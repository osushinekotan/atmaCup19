{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import rootutils\n",
    "import scipy.sparse as sp\n",
    "import seaborn as sns\n",
    "from jpholiday import JPHoliday\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "ROOT = rootutils.setup_root(\".\", pythonpath=True, cwd=True)\n",
    "\n",
    "from src.feature.tabular import AggregateEncoder, OrdinalEncoder, RawEncoder\n",
    "from src.feature.utils import cache\n",
    "from src.model.sklearn_like import LightGBMWapper\n",
    "from src.trainer.tabular.simple import single_inference_fn_v2, single_train_fn\n",
    "\n",
    "pl.Config.set_fmt_str_lengths(200)\n",
    "pl.Config.set_tbl_cols(50)\n",
    "pl.Config.set_tbl_rows(50)\n",
    "\n",
    "jpholiday = JPHoliday()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"032\"\n",
    "\n",
    "USE_FE_CACHE = True\n",
    "\n",
    "SEED = 42\n",
    "SEEDS = [x + SEED for x in range(3)]  # seed averaging\n",
    "N_SPLITS = 1  # (N_SPLITS = 1): holdout\n",
    "\n",
    "DEBUG = False\n",
    "VALID_SAMPLE_FRAC = 1\n",
    "VALID_DATE = \"2024-10-01\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = ROOT / \"data\"\n",
    "INPUT_DIR = DATA_DIR / \"atmacup19_dataset\"\n",
    "OUTPUT_DIR = DATA_DIR / \"output\"\n",
    "CACHE_DIR = DATA_DIR / \"cache\" / EXP_NAME\n",
    "\n",
    "for d in [DATA_DIR, INPUT_DIR, OUTPUT_DIR, CACHE_DIR]:\n",
    "    d.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "EXP_OUTPUT_DIR = OUTPUT_DIR / EXP_NAME\n",
    "EXP_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FEATURE_PREFIX = \"f_\"\n",
    "NUMERICAL_FEATURE_PREFIX = f\"{FEATURE_PREFIX}n_\"\n",
    "CATEGORICAL_FEATURE_PREFIX = f\"{FEATURE_PREFIX}c_\"\n",
    "\n",
    "TARGET_COLS = [\n",
    "    \"ヘアケア\",\n",
    "    \"チョコレート\",\n",
    "    \"ビール\",\n",
    "    \"米（5㎏以下）\",\n",
    "]\n",
    "META_COLS = [\n",
    "    \"session_id\",\n",
    "    \"顧客CD\",\n",
    "    \"session_datetime\",\n",
    "    \"original_顧客CD\",\n",
    "] + TARGET_COLS\n",
    "\n",
    "FOLD_COL = \"fold\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec_log_df = pl.read_csv(INPUT_DIR / \"ec_log.csv\", infer_schema_length=200000)\n",
    "jan_df = pl.read_csv(INPUT_DIR / \"jan.csv\")\n",
    "test_session_df = pl.read_csv(INPUT_DIR / \"test_session.csv\")\n",
    "train_session_df = pl.read_csv(INPUT_DIR / \"train_session.csv\")\n",
    "train_log_df = pl.read_csv(INPUT_DIR / \"train_log.csv\")\n",
    "\n",
    "# create target\n",
    "train_session_df = (\n",
    "    train_session_df.select(\"session_id\")\n",
    "    .join(train_log_df, on=\"session_id\", how=\"left\")\n",
    "    .join(jan_df.filter(pl.col(\"カテゴリ名\").is_in(TARGET_COLS)), on=\"JAN\", how=\"left\")\n",
    "    .select([\"session_id\", \"カテゴリ名\", \"売上数量\"])\n",
    "    .group_by([\"session_id\", \"カテゴリ名\"])\n",
    "    .agg(pl.col(\"売上数量\").sum())\n",
    "    .filter(pl.col(\"売上数量\") > 0)\n",
    "    .with_columns(pl.lit(1).alias(\"target\"))\n",
    "    .pivot(\"カテゴリ名\", index=\"session_id\", values=\"target\")\n",
    "    .select([\"session_id\"] + TARGET_COLS)\n",
    "    .join(train_session_df, on=\"session_id\", how=\"right\")\n",
    "    .with_columns(pl.col(x).fill_null(0) for x in TARGET_COLS)\n",
    ")\n",
    "\n",
    "\n",
    "valid_session_id = (\n",
    "    train_session_df.sort(\"売上日\")\n",
    "    .group_by(\"顧客CD\")\n",
    "    .tail(1)\n",
    "    .filter(pl.col(\"売上日\") >= VALID_DATE)[\"session_id\"]\n",
    "    .unique()\n",
    "    .sample(fraction=VALID_SAMPLE_FRAC, seed=SEED)\n",
    ")\n",
    "valid_session_df = train_session_df.filter(pl.col(\"session_id\").is_in(valid_session_id))\n",
    "train_session_df = train_session_df.filter(pl.col(\"session_id\").is_in(valid_session_id).not_())\n",
    "\n",
    "# sampling\n",
    "if DEBUG:\n",
    "    train_session_df = train_session_df.sample(10000, seed=SEED)\n",
    "\n",
    "\n",
    "raw_full_session_df = (\n",
    "    pl.concat(\n",
    "        [\n",
    "            train_session_df.with_columns(dataset=pl.lit(\"TRAIN\")),\n",
    "            valid_session_df.with_columns(dataset=pl.lit(\"VALID\")),\n",
    "            test_session_df.with_columns(dataset=pl.lit(\"TEST\")),\n",
    "        ],\n",
    "        how=\"diagonal_relaxed\",\n",
    "    )\n",
    "    .with_columns(pl.col(\"売上日\").cast(pl.Date))\n",
    "    .with_columns(\n",
    "        pl.datetime(\n",
    "            pl.col(\"売上日\").dt.year(), pl.col(\"売上日\").dt.month(), pl.col(\"売上日\").dt.day(), pl.col(\"時刻\")\n",
    "        ).alias(\"session_datetime\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"train_session_df: {train_session_df.shape}\")\n",
    "print(f\"valid_session_df: {valid_session_df.shape}\")\n",
    "print(f\"test_session_df: {test_session_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_df = pl.DataFrame(\n",
    "    [\n",
    "        {\"売上日\": (x.date), \"name\": x.name, \"is_holiday\": 1}\n",
    "        for x in jpholiday.between(datetime.date(2024, 7, 1), datetime.date(2024, 11, 30))\n",
    "    ]\n",
    ")\n",
    "\n",
    "full_session_df = raw_full_session_df.with_columns(pl.col(\"顧客CD\").alias(\"original_顧客CD\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cache(cache_dir=CACHE_DIR, overwrite=not USE_FE_CACHE)\n",
    "def get_top_co_categories(\n",
    "    train_log_df: pl.DataFrame,\n",
    "    jan_df: pl.DataFrame,\n",
    "    n_top_per_target: int = 100,\n",
    "    target_categories: list[str] = [\"チョコレート\", \"ビール\", \"米（5㎏以下）\", \"ヘアケア\"],  # noqa\n",
    ") -> list[str]:\n",
    "    print(\"🚀 get_top_co_categories\")\n",
    "    all_top_co_categories = set()\n",
    "\n",
    "    for target_category in target_categories:\n",
    "        top_co_categories = (\n",
    "            train_log_df.lazy()\n",
    "            .join(jan_df.lazy().select([\"JAN\", \"カテゴリ名\"]), on=\"JAN\", how=\"inner\")\n",
    "            .drop(\"JAN\")\n",
    "            .unique()\n",
    "            .group_by(\"session_id\")\n",
    "            .agg(pl.col(\"カテゴリ名\"))\n",
    "            .filter(pl.col(\"カテゴリ名\").list.contains(target_category))\n",
    "            .explode(\"カテゴリ名\")\n",
    "            .unique()\n",
    "            .group_by(\"カテゴリ名\")\n",
    "            .agg(pl.len().alias(\"n_cooccurrence\"))\n",
    "            .sort(\"n_cooccurrence\", descending=True)\n",
    "            .limit(n_top_per_target)\n",
    "            .collect()[\"カテゴリ名\"]\n",
    "            .to_list()\n",
    "        )\n",
    "\n",
    "        all_top_co_categories.update(top_co_categories)\n",
    "\n",
    "    all_top_co_categories.update(target_categories)\n",
    "    return sorted(list(all_top_co_categories))\n",
    "\n",
    "\n",
    "@cache(cache_dir=CACHE_DIR, overwrite=not USE_FE_CACHE)\n",
    "def create_session_duration_last_cat_df(\n",
    "    train_log_df: pl.DataFrame,\n",
    "    jan_df: pl.DataFrame,\n",
    "    session_df: pl.DataFrame,\n",
    "    prefix: str = \"days_since_cat_\",\n",
    "    target_categories: list[str] = [\"チョコレート\", \"ビール\", \"米（5㎏以下）\", \"ヘアケア\"],  # noqa\n",
    "    cat_type: str = \"カテゴリ名\",\n",
    "    group_by: str | list[str] = \"顧客CD\",\n",
    ") -> pl.DataFrame:\n",
    "    print(\"🚀 create_session_duration_last_cat_df\")\n",
    "    print(f\"# target_categories: {len(target_categories)}\")\n",
    "    group_by = [group_by] if isinstance(group_by, str) else group_by\n",
    "    result_df = (\n",
    "        (\n",
    "            (\n",
    "                session_df.lazy()\n",
    "                .join(train_log_df.lazy(), on=\"session_id\", how=\"left\")\n",
    "                .join(jan_df.lazy().select([\"JAN\", cat_type]), on=\"JAN\", how=\"left\")\n",
    "                .select([\"session_id\", \"session_datetime\", \"売上数量\", cat_type, *group_by])\n",
    "                .unique()\n",
    "            )\n",
    "            .group_by([\"session_id\", \"session_datetime\", *group_by])  # 顧客CDごとにグループ化\n",
    "            .agg(pl.col(cat_type))\n",
    "            .with_columns(\n",
    "                pl.col(cat_type).list.contains(target_category).alias(target_category)\n",
    "                for target_category in target_categories\n",
    "            )\n",
    "            .sort(\n",
    "                [*group_by, \"session_datetime\", \"session_id\"]\n",
    "            )  # 顧客CDごとに時系列でソート: 同一時間/別 session_id をどうするか問題\n",
    "        )\n",
    "        .with_columns(\n",
    "            # 対象カテゴリを購入した場合のみ日時を記録\n",
    "            [\n",
    "                pl.when(pl.col(target_category))\n",
    "                .then(pl.col(\"session_datetime\"))\n",
    "                .otherwise(None)\n",
    "                .alias(f\"true_datetime_{target_category}\")\n",
    "                for target_category in target_categories\n",
    "            ]\n",
    "        )\n",
    "        .with_columns(\n",
    "            [\n",
    "                pl.col(f\"true_datetime_{target_category}\")\n",
    "                .forward_fill()\n",
    "                .shift()\n",
    "                .over(group_by)  # 顧客CDごとのウィンドウで処理\n",
    "                .alias(f\"last_true_datetime_{target_category}\")\n",
    "                for target_category in target_categories\n",
    "            ]\n",
    "        )\n",
    "        .with_columns(\n",
    "            [\n",
    "                pl.when(pl.col(f\"last_true_datetime_{target_category}\").is_not_null())\n",
    "                .then(\n",
    "                    (pl.col(\"session_datetime\") - pl.col(f\"last_true_datetime_{target_category}\")) / pl.duration(days=1)\n",
    "                )\n",
    "                .otherwise(None)\n",
    "                .alias(f\"{prefix}_{target_category}\")\n",
    "                for target_category in target_categories\n",
    "            ]\n",
    "        )\n",
    "        .select(\n",
    "            pl.col(\"session_id\"),\n",
    "            *[pl.col(f\"{prefix}_{target_category}\") for target_category in target_categories],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return result_df.collect()\n",
    "\n",
    "\n",
    "@cache(cache_dir=CACHE_DIR, overwrite=not USE_FE_CACHE)\n",
    "def create_window_agg_cat_df(\n",
    "    session_df: pl.DataFrame,\n",
    "    train_log_df: pl.DataFrame,\n",
    "    jan_df: pl.DataFrame,\n",
    "    target_categories: list[str] = [\"チョコレート\", \"ビール\", \"米（5㎏以下）\", \"ヘアケア\"],  # noqa\n",
    "    window_size: str | int = \"3mo\",\n",
    "    agg_method: str = \"mean\",  # [mean, sum, max, diff, shift]\n",
    "    value_type: str = \"売上有無\",  # [売上有無, 売上数量, 売上金額, 値割金額, 値割数量]\n",
    "    cat_type: str = \"カテゴリ名\",  # [カテゴリ名, 部門]\n",
    "    prefix: str = \"window_agg_cat_\",\n",
    "    group_by: str | list[str] = \"顧客CD\",\n",
    ") -> pl.DataFrame:\n",
    "    group_by = [group_by] if isinstance(group_by, str) else group_by\n",
    "    jan_cols = [\"JAN\"] if cat_type == \"JAN\" else [\"JAN\", cat_type]\n",
    "    base_df = (\n",
    "        session_df.lazy()\n",
    "        .join(train_log_df.lazy().with_columns(pl.lit(1).alias(\"売上有無\")), on=\"session_id\", how=\"left\")\n",
    "        .join(\n",
    "            jan_df.filter(pl.col(cat_type).is_in(target_categories)).lazy().select(jan_cols),\n",
    "            on=\"JAN\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "        .select([\"session_id\", \"session_datetime\", cat_type, *group_by, value_type])\n",
    "        .unique()\n",
    "        .collect()\n",
    "        .pivot(\n",
    "            cat_type,\n",
    "            index=[*group_by, \"session_id\", \"session_datetime\"],\n",
    "            values=value_type,\n",
    "            aggregate_function=\"sum\",\n",
    "        )\n",
    "        .fill_null(0)\n",
    "    )\n",
    "    available_categories = [x for x in target_categories if x in base_df.columns]\n",
    "    base_df = base_df.select(\n",
    "        [pl.col(\"session_id\"), pl.col(\"session_datetime\")] + group_by + available_categories\n",
    "    ).lazy()\n",
    "\n",
    "    if agg_method == \"mean\":\n",
    "        agg_df = base_df.sort([*group_by, \"session_datetime\"]).select(\n",
    "            pl.col(\"session_id\"),\n",
    "            *[\n",
    "                pl.col(target)\n",
    "                .rolling_mean_by(\"session_datetime\", window_size=window_size, closed=\"left\")\n",
    "                .over(group_by)\n",
    "                .alias(f\"{prefix}_mean_{target}_{value_type}_{window_size}\")\n",
    "                for target in available_categories\n",
    "            ],\n",
    "        )\n",
    "    elif agg_method == \"sum\":\n",
    "        agg_df = base_df.sort([*group_by, \"session_datetime\"]).select(\n",
    "            pl.col(\"session_id\"),\n",
    "            *[\n",
    "                pl.col(target)\n",
    "                .rolling_sum_by(\"session_datetime\", window_size=window_size, closed=\"left\")\n",
    "                .over(group_by)\n",
    "                .alias(f\"{prefix}_sum_{target}_{value_type}_{window_size}\")\n",
    "                for target in available_categories\n",
    "            ],\n",
    "        )\n",
    "    elif agg_method == \"max\":\n",
    "        agg_df = base_df.sort([*group_by, \"session_datetime\"]).select(\n",
    "            pl.col(\"session_id\"),\n",
    "            *[\n",
    "                pl.col(target)\n",
    "                .rolling_max_by(\"session_datetime\", window_size=window_size, closed=\"left\")\n",
    "                .over(group_by)\n",
    "                .alias(f\"{prefix}_max_{target}_{value_type}_{window_size}\")\n",
    "                for target in available_categories\n",
    "            ],\n",
    "        )\n",
    "    elif agg_method == \"min\":\n",
    "        agg_df = base_df.sort([*group_by, \"session_datetime\"]).select(\n",
    "            pl.col(\"session_id\"),\n",
    "            *[\n",
    "                pl.col(target)\n",
    "                .rolling_min_by(\"session_datetime\", window_size=window_size, closed=\"left\")\n",
    "                .over(group_by)\n",
    "                .alias(f\"{prefix}_min_{target}_{value_type}_{window_size}\")\n",
    "                for target in available_categories\n",
    "            ],\n",
    "        )\n",
    "    elif agg_method == \"std\":\n",
    "        agg_df = base_df.sort([*group_by, \"session_datetime\"]).select(\n",
    "            pl.col(\"session_id\"),\n",
    "            *[\n",
    "                pl.col(target)\n",
    "                .rolling_std_by(\"session_datetime\", window_size=window_size, closed=\"left\")\n",
    "                .over(group_by)\n",
    "                .alias(f\"{prefix}_std_{target}_{value_type}_{window_size}\")\n",
    "                for target in available_categories\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    elif agg_method == \"diff\":\n",
    "        assert isinstance(window_size, int), \"window_size must be int\"\n",
    "        agg_df = base_df.sort([*group_by, \"session_datetime\"]).select(\n",
    "            pl.col(\"session_id\"),\n",
    "            *[\n",
    "                pl.col(target)\n",
    "                .diff(n=window_size)\n",
    "                .over(group_by)\n",
    "                .alias(f\"{prefix}_diff_{target}_{value_type}_{window_size}\")\n",
    "                for target in available_categories\n",
    "            ],\n",
    "        )\n",
    "    elif agg_method == \"shift\":\n",
    "        assert isinstance(window_size, int), \"window_size must be int\"\n",
    "        agg_df = base_df.sort([*group_by, \"session_datetime\"]).select(\n",
    "            pl.col(\"session_id\"),\n",
    "            *[\n",
    "                pl.col(target)\n",
    "                .shift(n=window_size)\n",
    "                .over(group_by)\n",
    "                .alias(f\"{prefix}_shift_{target}_{value_type}_{window_size}\")\n",
    "                for target in available_categories\n",
    "            ],\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid agg_method: {agg_method}\")\n",
    "\n",
    "    return agg_df.collect()\n",
    "\n",
    "\n",
    "@cache(cache_dir=CACHE_DIR, overwrite=not USE_FE_CACHE)\n",
    "def create_session_embedding_df(\n",
    "    train_log_df: pl.DataFrame,\n",
    "    session_df: pl.DataFrame,\n",
    "    jan_df: pl.DataFrame,\n",
    "    item_type: str = \"カテゴリ名\",\n",
    "    value_type: str = \"売上数量\",\n",
    "    dim: int = 32,\n",
    "    window_size: str | int = \"6mo\",\n",
    "    agg_method: str = \"sum\",\n",
    "    group_by: str | list[str] = \"顧客CD\",\n",
    "    prefix: str = \"session_embedding_\",\n",
    ") -> pl.DataFrame:\n",
    "    print(\"🚀 create_session_embedding_df\")\n",
    "    noleak_mtx_df = (\n",
    "        create_window_agg_cat_df(\n",
    "            session_df=session_df,\n",
    "            train_log_df=train_log_df,\n",
    "            jan_df=jan_df,\n",
    "            target_categories=jan_df[item_type].unique().to_list(),\n",
    "            window_size=window_size,\n",
    "            agg_method=agg_method,\n",
    "            cat_type=item_type,\n",
    "            value_type=value_type,\n",
    "            group_by=group_by,\n",
    "            prefix=\"\",\n",
    "        )\n",
    "        .fill_null(0)\n",
    "        .select(pl.all().shrink_dtype())\n",
    "    )\n",
    "\n",
    "    print(f\"TruncatedSVD: dim={dim}\")\n",
    "    svd = TruncatedSVD(n_components=dim, random_state=42)\n",
    "    session_embeddings = svd.fit_transform(sp.csr_matrix(noleak_mtx_df.drop(\"session_id\").to_numpy()))\n",
    "    embedding_df = pl.DataFrame(\n",
    "        session_embeddings,\n",
    "        schema=[\n",
    "            f\"{prefix}_{item_type}_{value_type}_{window_size}_{agg_method}_d{dim}_{i + 1:03}\"\n",
    "            for i in range(session_embeddings.shape[1])\n",
    "        ],\n",
    "    ).with_columns(pl.Series(name=\"session_id\", values=noleak_mtx_df[\"session_id\"]))\n",
    "    return embedding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_duration_last_cat_df = create_session_duration_last_cat_df(\n",
    "    train_log_df=train_log_df,\n",
    "    jan_df=jan_df,\n",
    "    session_df=full_session_df,\n",
    "    target_categories=get_top_co_categories(\n",
    "        train_log_df,\n",
    "        jan_df,\n",
    "        n_top_per_target=128,\n",
    "        target_categories=[\"チョコレート\", \"ビール\", \"米（5㎏以下）\", \"ヘアケア\"],\n",
    "    ),\n",
    "    cat_type=\"カテゴリ名\",\n",
    ")\n",
    "\n",
    "master_session_embedding_df = full_session_df.select(\"session_id\")\n",
    "for window_size, agg_method, value_type in tqdm(\n",
    "    [\n",
    "        # window_size, agg_method, value_type\n",
    "        (\"6mo\", \"sum\", \"売上数量\"),\n",
    "        # (\"1mo\", \"sum\", \"売上数量\"),\n",
    "        (\"1w\", \"sum\", \"売上数量\"),\n",
    "        (\"3d\", \"sum\", \"売上数量\"),\n",
    "        # (\"6mo\", \"max\", \"売上数量\"),\n",
    "        # (\"1mo\", \"max\", \"売上数量\"),\n",
    "        # (\"1w\", \"max\", \"売上数量\"),\n",
    "        # (\"3d\", \"max\", \"売上数量\"),\n",
    "        # (\"6mo\", \"min\", \"売上数量\"),\n",
    "        # (\"1mo\", \"min\", \"売上数量\"),\n",
    "        # (\"1w\", \"min\", \"売上数量\"),\n",
    "        # (\"3d\", \"min\", \"売上数量\"),\n",
    "    ]\n",
    "):\n",
    "    master_session_embedding_df = master_session_embedding_df.join(\n",
    "        create_session_embedding_df(\n",
    "            train_log_df=train_log_df,\n",
    "            session_df=full_session_df,\n",
    "            jan_df=jan_df,\n",
    "            item_type=\"カテゴリ名\",\n",
    "            value_type=value_type,\n",
    "            dim=128,\n",
    "            window_size=window_size,\n",
    "            agg_method=agg_method,\n",
    "            prefix=\"session_embedding_v1_\",\n",
    "        ),\n",
    "        on=\"session_id\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    master_session_embedding_df = master_session_embedding_df.join(\n",
    "        create_session_embedding_df(\n",
    "            train_log_df=train_log_df,\n",
    "            session_df=full_session_df,\n",
    "            jan_df=jan_df,\n",
    "            item_type=\"カテゴリ名\",\n",
    "            value_type=value_type,\n",
    "            dim=128,\n",
    "            window_size=window_size,\n",
    "            agg_method=agg_method,\n",
    "            group_by=[\"年代\", \"性別\", \"店舗名\"],\n",
    "            prefix=\"session_embedding_v2_\",\n",
    "        ),\n",
    "        on=\"session_id\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "master_rolling_agg_cat_df = full_session_df.select(\"session_id\")\n",
    "for window_size, agg_method, value_type in tqdm(\n",
    "    [\n",
    "        # window_size, agg_method, value_type\n",
    "        (\"6mo\", \"sum\", \"売上数量\"),\n",
    "        # (\"1mo\", \"sum\", \"売上数量\"),\n",
    "        (\"1w\", \"sum\", \"売上数量\"),\n",
    "        (\"3d\", \"sum\", \"売上数量\"),\n",
    "        (\"6mo\", \"mean\", \"売上数量\"),\n",
    "        # (\"1mo\", \"mean\", \"売上数量\"),\n",
    "        (\"1w\", \"mean\", \"売上数量\"),\n",
    "        (\"3d\", \"mean\", \"売上数量\"),\n",
    "        (\"6mo\", \"max\", \"売上数量\"),\n",
    "        # (\"1mo\", \"max\", \"売上数量\"),\n",
    "        (\"1w\", \"max\", \"売上数量\"),\n",
    "        (\"3d\", \"max\", \"売上数量\"),\n",
    "        (\"6mo\", \"std\", \"売上数量\"),\n",
    "        # (\"1mo\", \"std\", \"売上数量\"),\n",
    "        (\"1w\", \"std\", \"売上数量\"),\n",
    "        (\"3d\", \"std\", \"売上数量\"),\n",
    "        (\"6mo\", \"max\", \"値割数量\"),\n",
    "        (\"6mo\", \"sum\", \"値割数量\"),\n",
    "        (\"6mo\", \"mean\", \"値割数量\"),\n",
    "        (\"6mo\", \"std\", \"値割数量\"),\n",
    "    ]\n",
    "):\n",
    "    rolling_agg_df = create_window_agg_cat_df(\n",
    "        session_df=full_session_df,\n",
    "        train_log_df=train_log_df,\n",
    "        jan_df=jan_df,\n",
    "        target_categories=get_top_co_categories(\n",
    "            train_log_df,\n",
    "            jan_df,\n",
    "            n_top_per_target=16,\n",
    "            target_categories=[\"チョコレート\", \"ビール\", \"米（5㎏以下）\", \"ヘアケア\"],\n",
    "        ),\n",
    "        window_size=window_size,\n",
    "        agg_method=agg_method,\n",
    "        value_type=value_type,\n",
    "        cat_type=\"カテゴリ名\",\n",
    "    )\n",
    "\n",
    "    master_rolling_agg_cat_df = master_rolling_agg_cat_df.join(\n",
    "        rolling_agg_df,\n",
    "        on=\"session_id\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "\n",
    "# add feature\n",
    "full_session_df_with_feats = (\n",
    "    (\n",
    "        full_session_df.with_columns(pl.lit(1).alias(\"dummy\"))\n",
    "        .with_columns(\n",
    "            pl.col(\"顧客CD\")\n",
    "            .is_in(full_session_df.filter(pl.col(\"dataset\") != \"TRAIN\")[\"顧客CD\"].unique())\n",
    "            .alias(\"is_hot\"),\n",
    "            pl.col(\"売上日\").dt.day().alias(\"day\"),\n",
    "            pl.col(\"売上日\").dt.weekday().alias(\"weekday\"),\n",
    "            pl.col(\"年代\")\n",
    "            .replace(\n",
    "                {\n",
    "                    \"10代以下\": 10,\n",
    "                    \"20代\": 20,\n",
    "                    \"30代\": 30,\n",
    "                    \"40代\": 40,\n",
    "                    \"50代\": 50,\n",
    "                    \"60代\": 60,\n",
    "                    \"70代\": 70,\n",
    "                    \"80代以上\": 80,\n",
    "                    \"不明\": None,\n",
    "                }\n",
    "            )\n",
    "            .cast(pl.Float32)\n",
    "            .alias(\"age\"),\n",
    "            pl.col(\"session_id\").cum_count().over(\"顧客CD\").alias(\"cum_visit_count\"),\n",
    "            pl.col(\"session_datetime\").diff().over(\"顧客CD\").alias(\"days_since_last_visit\").dt.total_days(),\n",
    "            pl.col(\"dummy\")\n",
    "            .rolling_sum_by(\"session_datetime\", window_size=\"1mo\")\n",
    "            .over(\"顧客CD\")\n",
    "            .alias(\"visit_count_1mo\"),\n",
    "            pl.col(\"dummy\").rolling_sum_by(\"session_datetime\", window_size=\"1w\").over(\"顧客CD\").alias(\"visit_count_1w\"),\n",
    "            pl.col(\"dummy\").sum().over([\"顧客CD\", \"売上日\"]).alias(\"visit_count_today\"),\n",
    "            pl.col(\"dummy\").sum().over([\"顧客CD\"]).alias(\"visit_count_total\"),\n",
    "            pl.when(pl.col(\"時刻\").is_between(8, 12, closed=\"both\")).then(0).otherwise(1).alias(\"is_am\"),\n",
    "        )\n",
    "        .drop(\"dummy\")\n",
    "    )\n",
    "    .join(holiday_df, on=\"売上日\", how=\"left\")\n",
    "    .with_columns(pl.col(\"is_holiday\").fill_null(0))\n",
    "    .join(session_duration_last_cat_df, on=\"session_id\", how=\"left\")\n",
    "    .join(master_rolling_agg_cat_df, on=\"session_id\", how=\"left\")\n",
    "    .join(master_session_embedding_df, on=\"session_id\", how=\"left\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fe(\n",
    "    train_df: pl.DataFrame,\n",
    "    test_df: pl.DataFrame,\n",
    "    valid_df: pl.DataFrame | None = None,\n",
    ") -> tuple[pl.DataFrame, pl.DataFrame]:\n",
    "    encoders = [\n",
    "        RawEncoder(\n",
    "            columns=META_COLS,\n",
    "            prefix=\"\",\n",
    "        ),\n",
    "        RawEncoder(\n",
    "            columns=[\n",
    "                \"時刻\",\n",
    "                \"day\",\n",
    "                \"weekday\",\n",
    "                \"age\",\n",
    "                \"cum_visit_count\",\n",
    "                \"days_since_last_visit\",\n",
    "                \"visit_count_1mo\",\n",
    "                \"visit_count_1w\",\n",
    "                \"visit_count_today\",\n",
    "                \"visit_count_total\",\n",
    "                \"is_hot\",\n",
    "                \"is_holiday\",\n",
    "                # \"is_am\",\n",
    "                *[x for x in train_df.columns if x.startswith(\"days_since_cat_\")],\n",
    "                *[x for x in train_df.columns if x.startswith(\"window_agg_cat_\")],\n",
    "                *[x for x in train_df.columns if x.startswith(\"session_embedding_\")],\n",
    "            ],\n",
    "            prefix=NUMERICAL_FEATURE_PREFIX,\n",
    "        ),\n",
    "        OrdinalEncoder(\n",
    "            columns=[\n",
    "                \"性別\",\n",
    "                \"顧客CD\",\n",
    "                \"店舗名\",\n",
    "            ],\n",
    "            prefix=CATEGORICAL_FEATURE_PREFIX,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # train, test に transform\n",
    "    train_feature_df = pl.concat(\n",
    "        [encoder.fit_transform(train_df) for encoder in encoders],\n",
    "        how=\"horizontal\",\n",
    "    ).select(pl.all().shrink_dtype())\n",
    "\n",
    "    test_feature_df = pl.concat(\n",
    "        [encoder.transform(test_df) for encoder in encoders],\n",
    "        how=\"horizontal\",\n",
    "    ).select(pl.all().shrink_dtype())\n",
    "\n",
    "    if valid_df is not None:\n",
    "        valid_feature_df = pl.concat(\n",
    "            [encoder.transform(valid_df) for encoder in encoders],\n",
    "            how=\"horizontal\",\n",
    "        ).select(pl.all().shrink_dtype())\n",
    "        return train_feature_df, test_feature_df, valid_feature_df\n",
    "\n",
    "    return train_feature_df, test_feature_df\n",
    "\n",
    "\n",
    "train_df = full_session_df_with_feats.filter(pl.col(\"dataset\") == \"TRAIN\")\n",
    "valid_df = full_session_df_with_feats.filter(pl.col(\"dataset\") == \"VALID\")\n",
    "test_df = full_session_df_with_feats.filter(pl.col(\"dataset\") == \"TEST\")\n",
    "\n",
    "train_feature_df, test_feature_df, valid_feature_df = fe(train_df, test_df, valid_df=valid_df)\n",
    "\n",
    "cat_feature_cols = [x for x in train_feature_df.columns if x.startswith(CATEGORICAL_FEATURE_PREFIX)]\n",
    "num_feature_cols = [x for x in train_feature_df.columns if x.startswith(NUMERICAL_FEATURE_PREFIX)]\n",
    "feature_cols = cat_feature_cols + num_feature_cols\n",
    "\n",
    "print(f\"numerical features: {len(num_feature_cols)}\")\n",
    "print(f\"categorical features: {len(cat_feature_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalFn:\n",
    "    def __init__(self, target_col: str, pred_col: str = \"pred\"):\n",
    "        self.target_col = target_col\n",
    "\n",
    "    def __call__(self, input_df: pl.DataFrame) -> dict[str, float]:\n",
    "        y_true = input_df[self.target_col].to_numpy()\n",
    "        y_pred = input_df[\"pred\"].to_numpy()\n",
    "\n",
    "        scores = {\n",
    "            \"rocauc\": roc_auc_score(y_true, y_pred),\n",
    "        }\n",
    "        return scores\n",
    "\n",
    "    @property\n",
    "    def __name__(self) -> str:\n",
    "        return self.__class__.__name__\n",
    "\n",
    "\n",
    "def add_kfold(\n",
    "    input_df: pl.DataFrame,\n",
    "    n_splits: int,\n",
    "    random_state: int,\n",
    "    fold_col: str,\n",
    ") -> pl.DataFrame:\n",
    "    if n_splits == 1:\n",
    "        return input_df.with_columns(pl.lit(0).alias(fold_col))\n",
    "\n",
    "    skf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)  # NOTE:gkf にするべき?\n",
    "    folds = np.zeros(len(input_df), dtype=np.int32)\n",
    "    for fold, (_, valid_idx) in enumerate(skf.split(X=input_df)):\n",
    "        folds[valid_idx] = fold\n",
    "    return input_df.with_columns(pl.Series(name=fold_col, values=folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, scores = {}, {}\n",
    "\n",
    "for target_col in TARGET_COLS:\n",
    "    va_result_df, va_scores = pl.DataFrame(), {}\n",
    "    all_models = []\n",
    "    for seed in SEEDS:\n",
    "        print(f\"# ================== seed={seed}: {target_col} ================== #\")\n",
    "        name = f\"seed{seed}_lgb_{target_col}\"\n",
    "        _va_result_df, _va_scores, trained_models = single_train_fn(\n",
    "            model=LightGBMWapper(\n",
    "                name=name,\n",
    "                model=lgb.LGBMModel(\n",
    "                    objective=\"binary\",\n",
    "                    boosting=\"gbdt\",\n",
    "                    n_estimators=10000,\n",
    "                    learning_rate=0.01,\n",
    "                    num_leaves=31,\n",
    "                    colsample_bytree=0.1,\n",
    "                    subsample=0.1,\n",
    "                    importance_type=\"gain\",\n",
    "                    random_state=seed,\n",
    "                    force_col_wise=True,\n",
    "                    class_weight=\"balanced\",\n",
    "                    # verbose=-1,\n",
    "                ),\n",
    "                fit_params={\n",
    "                    \"callbacks\": [\n",
    "                        lgb.early_stopping(100, first_metric_only=True),\n",
    "                        lgb.log_evaluation(period=100),\n",
    "                    ],\n",
    "                    \"categorical_feature\": cat_feature_cols,\n",
    "                    \"feature_name\": feature_cols,\n",
    "                    \"eval_metric\": \"auc\",\n",
    "                },\n",
    "            ),\n",
    "            features_df=add_kfold(\n",
    "                train_feature_df,\n",
    "                n_splits=N_SPLITS,\n",
    "                random_state=seed,\n",
    "                fold_col=FOLD_COL,\n",
    "            ),\n",
    "            feature_cols=feature_cols,\n",
    "            target_col=target_col,\n",
    "            fold_col=FOLD_COL,\n",
    "            meta_cols=META_COLS + [FOLD_COL],\n",
    "            out_dir=EXP_OUTPUT_DIR,\n",
    "            eval_fn=EvalFn(target_col=target_col),\n",
    "            overwrite=True,\n",
    "            val_features_df=valid_feature_df,\n",
    "            full_training=True,\n",
    "        )\n",
    "        va_result_df = pl.concat([va_result_df, _va_result_df], how=\"diagonal_relaxed\")\n",
    "        va_scores[name] = _va_scores\n",
    "        all_models.extend(trained_models)\n",
    "\n",
    "    va_result_agg_df = (\n",
    "        va_result_df.group_by(\"session_id\")\n",
    "        .agg(pl.col(\"pred\").mean())\n",
    "        .sort(\"session_id\")\n",
    "        .join(va_result_df.select(META_COLS), on=\"session_id\", how=\"left\")\n",
    "    )\n",
    "    results[target_col] = {\n",
    "        \"result_df\": va_result_agg_df,\n",
    "        \"models\": all_models,\n",
    "    }\n",
    "    scores[target_col] = va_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_va_result_df(results: dict) -> pl.DataFrame:\n",
    "    result_dfs = {k: x[\"result_df\"].with_columns(pl.col(\"pred\").alias(f\"pred_{k}\")) for k, x in results.items()}\n",
    "\n",
    "    va_result_df = pl.DataFrame()\n",
    "    for i, (name, result_df) in enumerate(result_dfs.items()):\n",
    "        if i == 0:\n",
    "            i_df = result_df.select([\"session_id\", \"顧客CD\", f\"pred_{name}\", name])\n",
    "        else:\n",
    "            i_df = result_df.select([f\"pred_{name}\", name])\n",
    "        va_result_df = pl.concat([va_result_df, i_df], how=\"horizontal\")\n",
    "    return va_result_df\n",
    "\n",
    "\n",
    "va_result_df = construct_va_result_df(results)\n",
    "pred_cols = [f\"pred_{x}\" for x in TARGET_COLS]\n",
    "score = roc_auc_score(va_result_df[TARGET_COLS].to_numpy(), va_result_df[pred_cols].to_numpy(), average=\"macro\")\n",
    "scores[\"final_metric\"] = score\n",
    "\n",
    "va_result_df.write_parquet(EXP_OUTPUT_DIR / \"va_result_df.parquet\")\n",
    "with open(EXP_OUTPUT_DIR / \"scores.json\", \"w\") as f:\n",
    "    json.dump(scores, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(json.dumps(scores, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_result_df = pl.DataFrame()\n",
    "for i, (name, res) in enumerate(results.items()):\n",
    "    target_name = name\n",
    "    if i == 0:\n",
    "        cols = [pl.col(\"session_id\"), pl.col(\"pred\").alias(target_name)]\n",
    "    else:\n",
    "        cols = [pl.col(\"pred\").alias(target_name)]\n",
    "\n",
    "    _te_result_df = single_inference_fn_v2(\n",
    "        models=res[\"models\"],\n",
    "        features_df=test_feature_df,\n",
    "        feature_names=feature_cols,\n",
    "    ).select(cols)\n",
    "    te_result_df = pl.concat([te_result_df, _te_result_df], how=\"horizontal\")\n",
    "\n",
    "submission_df = (\n",
    "    test_session_df.select(\"session_id\")\n",
    "    .join(te_result_df, on=\"session_id\", how=\"left\")\n",
    "    .select([\"session_id\"] + TARGET_COLS)\n",
    ")\n",
    "\n",
    "submission_df.write_parquet(EXP_OUTPUT_DIR / \"te_result_df.parquet\")\n",
    "submission_df.select(\n",
    "    [\n",
    "        \"チョコレート\",\n",
    "        \"ビール\",\n",
    "        \"ヘアケア\",\n",
    "        \"米（5㎏以下）\",\n",
    "    ]\n",
    ").write_csv(EXP_OUTPUT_DIR / \"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
