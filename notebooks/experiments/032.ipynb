{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import rootutils\n",
    "import scipy.sparse as sp\n",
    "import seaborn as sns\n",
    "from jpholiday import JPHoliday\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "ROOT = rootutils.setup_root(\".\", pythonpath=True, cwd=True)\n",
    "\n",
    "from src.feature.tabular import AggregateEncoder, OrdinalEncoder, RawEncoder\n",
    "from src.feature.utils import cache\n",
    "from src.model.sklearn_like import LightGBMWapper\n",
    "from src.trainer.tabular.simple import single_inference_fn_v2, single_train_fn\n",
    "\n",
    "pl.Config.set_fmt_str_lengths(200)\n",
    "pl.Config.set_tbl_cols(50)\n",
    "pl.Config.set_tbl_rows(50)\n",
    "\n",
    "jpholiday = JPHoliday()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"032\"\n",
    "\n",
    "USE_FE_CACHE = True\n",
    "\n",
    "SEED = 42\n",
    "SEEDS = [x + SEED for x in range(3)]  # seed averaging\n",
    "N_SPLITS = 1  # (N_SPLITS = 1): holdout\n",
    "\n",
    "DEBUG = False\n",
    "VALID_SAMPLE_FRAC = 1\n",
    "VALID_DATE = \"2024-10-01\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = ROOT / \"data\"\n",
    "INPUT_DIR = DATA_DIR / \"atmacup19_dataset\"\n",
    "OUTPUT_DIR = DATA_DIR / \"output\"\n",
    "CACHE_DIR = DATA_DIR / \"cache\" / EXP_NAME\n",
    "\n",
    "for d in [DATA_DIR, INPUT_DIR, OUTPUT_DIR, CACHE_DIR]:\n",
    "    d.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "EXP_OUTPUT_DIR = OUTPUT_DIR / EXP_NAME\n",
    "EXP_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FEATURE_PREFIX = \"f_\"\n",
    "NUMERICAL_FEATURE_PREFIX = f\"{FEATURE_PREFIX}n_\"\n",
    "CATEGORICAL_FEATURE_PREFIX = f\"{FEATURE_PREFIX}c_\"\n",
    "\n",
    "TARGET_COLS = [\n",
    "    \"„Éò„Ç¢„Ç±„Ç¢\",\n",
    "    \"„ÉÅ„Éß„Ç≥„É¨„Éº„Éà\",\n",
    "    \"„Éì„Éº„É´\",\n",
    "    \"Á±≥Ôºà5„éè‰ª•‰∏ãÔºâ\",\n",
    "]\n",
    "META_COLS = [\n",
    "    \"session_id\",\n",
    "    \"È°ßÂÆ¢CD\",\n",
    "    \"session_datetime\",\n",
    "    \"original_È°ßÂÆ¢CD\",\n",
    "] + TARGET_COLS\n",
    "\n",
    "FOLD_COL = \"fold\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec_log_df = pl.read_csv(INPUT_DIR / \"ec_log.csv\", infer_schema_length=200000)\n",
    "jan_df = pl.read_csv(INPUT_DIR / \"jan.csv\")\n",
    "test_session_df = pl.read_csv(INPUT_DIR / \"test_session.csv\")\n",
    "train_session_df = pl.read_csv(INPUT_DIR / \"train_session.csv\")\n",
    "train_log_df = pl.read_csv(INPUT_DIR / \"train_log.csv\")\n",
    "\n",
    "# create target\n",
    "train_session_df = (\n",
    "    train_session_df.select(\"session_id\")\n",
    "    .join(train_log_df, on=\"session_id\", how=\"left\")\n",
    "    .join(jan_df.filter(pl.col(\"„Ç´„ÉÜ„Ç¥„É™Âêç\").is_in(TARGET_COLS)), on=\"JAN\", how=\"left\")\n",
    "    .select([\"session_id\", \"„Ç´„ÉÜ„Ç¥„É™Âêç\", \"Â£≤‰∏äÊï∞Èáè\"])\n",
    "    .group_by([\"session_id\", \"„Ç´„ÉÜ„Ç¥„É™Âêç\"])\n",
    "    .agg(pl.col(\"Â£≤‰∏äÊï∞Èáè\").sum())\n",
    "    .filter(pl.col(\"Â£≤‰∏äÊï∞Èáè\") > 0)\n",
    "    .with_columns(pl.lit(1).alias(\"target\"))\n",
    "    .pivot(\"„Ç´„ÉÜ„Ç¥„É™Âêç\", index=\"session_id\", values=\"target\")\n",
    "    .select([\"session_id\"] + TARGET_COLS)\n",
    "    .join(train_session_df, on=\"session_id\", how=\"right\")\n",
    "    .with_columns(pl.col(x).fill_null(0) for x in TARGET_COLS)\n",
    ")\n",
    "\n",
    "\n",
    "valid_session_id = (\n",
    "    train_session_df.sort(\"Â£≤‰∏äÊó•\")\n",
    "    .group_by(\"È°ßÂÆ¢CD\")\n",
    "    .tail(1)\n",
    "    .filter(pl.col(\"Â£≤‰∏äÊó•\") >= VALID_DATE)[\"session_id\"]\n",
    "    .unique()\n",
    "    .sample(fraction=VALID_SAMPLE_FRAC, seed=SEED)\n",
    ")\n",
    "valid_session_df = train_session_df.filter(pl.col(\"session_id\").is_in(valid_session_id))\n",
    "train_session_df = train_session_df.filter(pl.col(\"session_id\").is_in(valid_session_id).not_())\n",
    "\n",
    "# sampling\n",
    "if DEBUG:\n",
    "    train_session_df = train_session_df.sample(10000, seed=SEED)\n",
    "\n",
    "\n",
    "raw_full_session_df = (\n",
    "    pl.concat(\n",
    "        [\n",
    "            train_session_df.with_columns(dataset=pl.lit(\"TRAIN\")),\n",
    "            valid_session_df.with_columns(dataset=pl.lit(\"VALID\")),\n",
    "            test_session_df.with_columns(dataset=pl.lit(\"TEST\")),\n",
    "        ],\n",
    "        how=\"diagonal_relaxed\",\n",
    "    )\n",
    "    .with_columns(pl.col(\"Â£≤‰∏äÊó•\").cast(pl.Date))\n",
    "    .with_columns(\n",
    "        pl.datetime(\n",
    "            pl.col(\"Â£≤‰∏äÊó•\").dt.year(), pl.col(\"Â£≤‰∏äÊó•\").dt.month(), pl.col(\"Â£≤‰∏äÊó•\").dt.day(), pl.col(\"ÊôÇÂàª\")\n",
    "        ).alias(\"session_datetime\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"train_session_df: {train_session_df.shape}\")\n",
    "print(f\"valid_session_df: {valid_session_df.shape}\")\n",
    "print(f\"test_session_df: {test_session_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_df = pl.DataFrame(\n",
    "    [\n",
    "        {\"Â£≤‰∏äÊó•\": (x.date), \"name\": x.name, \"is_holiday\": 1}\n",
    "        for x in jpholiday.between(datetime.date(2024, 7, 1), datetime.date(2024, 11, 30))\n",
    "    ]\n",
    ")\n",
    "\n",
    "full_session_df = raw_full_session_df.with_columns(pl.col(\"È°ßÂÆ¢CD\").alias(\"original_È°ßÂÆ¢CD\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cache(cache_dir=CACHE_DIR, overwrite=not USE_FE_CACHE)\n",
    "def get_top_co_categories(\n",
    "    train_log_df: pl.DataFrame,\n",
    "    jan_df: pl.DataFrame,\n",
    "    n_top_per_target: int = 100,\n",
    "    target_categories: list[str] = [\"„ÉÅ„Éß„Ç≥„É¨„Éº„Éà\", \"„Éì„Éº„É´\", \"Á±≥Ôºà5„éè‰ª•‰∏ãÔºâ\", \"„Éò„Ç¢„Ç±„Ç¢\"],  # noqa\n",
    ") -> list[str]:\n",
    "    print(\"üöÄ get_top_co_categories\")\n",
    "    all_top_co_categories = set()\n",
    "\n",
    "    for target_category in target_categories:\n",
    "        top_co_categories = (\n",
    "            train_log_df.lazy()\n",
    "            .join(jan_df.lazy().select([\"JAN\", \"„Ç´„ÉÜ„Ç¥„É™Âêç\"]), on=\"JAN\", how=\"inner\")\n",
    "            .drop(\"JAN\")\n",
    "            .unique()\n",
    "            .group_by(\"session_id\")\n",
    "            .agg(pl.col(\"„Ç´„ÉÜ„Ç¥„É™Âêç\"))\n",
    "            .filter(pl.col(\"„Ç´„ÉÜ„Ç¥„É™Âêç\").list.contains(target_category))\n",
    "            .explode(\"„Ç´„ÉÜ„Ç¥„É™Âêç\")\n",
    "            .unique()\n",
    "            .group_by(\"„Ç´„ÉÜ„Ç¥„É™Âêç\")\n",
    "            .agg(pl.len().alias(\"n_cooccurrence\"))\n",
    "            .sort(\"n_cooccurrence\", descending=True)\n",
    "            .limit(n_top_per_target)\n",
    "            .collect()[\"„Ç´„ÉÜ„Ç¥„É™Âêç\"]\n",
    "            .to_list()\n",
    "        )\n",
    "\n",
    "        all_top_co_categories.update(top_co_categories)\n",
    "\n",
    "    all_top_co_categories.update(target_categories)\n",
    "    return sorted(list(all_top_co_categories))\n",
    "\n",
    "\n",
    "@cache(cache_dir=CACHE_DIR, overwrite=not USE_FE_CACHE)\n",
    "def create_session_duration_last_cat_df(\n",
    "    train_log_df: pl.DataFrame,\n",
    "    jan_df: pl.DataFrame,\n",
    "    session_df: pl.DataFrame,\n",
    "    prefix: str = \"days_since_cat_\",\n",
    "    target_categories: list[str] = [\"„ÉÅ„Éß„Ç≥„É¨„Éº„Éà\", \"„Éì„Éº„É´\", \"Á±≥Ôºà5„éè‰ª•‰∏ãÔºâ\", \"„Éò„Ç¢„Ç±„Ç¢\"],  # noqa\n",
    "    cat_type: str = \"„Ç´„ÉÜ„Ç¥„É™Âêç\",\n",
    "    group_by: str | list[str] = \"È°ßÂÆ¢CD\",\n",
    ") -> pl.DataFrame:\n",
    "    print(\"üöÄ create_session_duration_last_cat_df\")\n",
    "    print(f\"# target_categories: {len(target_categories)}\")\n",
    "    group_by = [group_by] if isinstance(group_by, str) else group_by\n",
    "    result_df = (\n",
    "        (\n",
    "            (\n",
    "                session_df.lazy()\n",
    "                .join(train_log_df.lazy(), on=\"session_id\", how=\"left\")\n",
    "                .join(jan_df.lazy().select([\"JAN\", cat_type]), on=\"JAN\", how=\"left\")\n",
    "                .select([\"session_id\", \"session_datetime\", \"Â£≤‰∏äÊï∞Èáè\", cat_type, *group_by])\n",
    "                .unique()\n",
    "            )\n",
    "            .group_by([\"session_id\", \"session_datetime\", *group_by])  # È°ßÂÆ¢CD„Åî„Å®„Å´„Ç∞„É´„Éº„ÉóÂåñ\n",
    "            .agg(pl.col(cat_type))\n",
    "            .with_columns(\n",
    "                pl.col(cat_type).list.contains(target_category).alias(target_category)\n",
    "                for target_category in target_categories\n",
    "            )\n",
    "            .sort(\n",
    "                [*group_by, \"session_datetime\", \"session_id\"]\n",
    "            )  # È°ßÂÆ¢CD„Åî„Å®„Å´ÊôÇÁ≥ªÂàó„Åß„ÇΩ„Éº„Éà: Âêå‰∏ÄÊôÇÈñì/Âà• session_id „Çí„Å©„ÅÜ„Åô„Çã„ÅãÂïèÈ°å\n",
    "        )\n",
    "        .with_columns(\n",
    "            # ÂØæË±°„Ç´„ÉÜ„Ç¥„É™„ÇíË≥ºÂÖ•„Åó„ÅüÂ†¥Âêà„ÅÆ„ÅøÊó•ÊôÇ„ÇíË®òÈå≤\n",
    "            [\n",
    "                pl.when(pl.col(target_category))\n",
    "                .then(pl.col(\"session_datetime\"))\n",
    "                .otherwise(None)\n",
    "                .alias(f\"true_datetime_{target_category}\")\n",
    "                for target_category in target_categories\n",
    "            ]\n",
    "        )\n",
    "        .with_columns(\n",
    "            [\n",
    "                pl.col(f\"true_datetime_{target_category}\")\n",
    "                .forward_fill()\n",
    "                .shift()\n",
    "                .over(group_by)  # È°ßÂÆ¢CD„Åî„Å®„ÅÆ„Ç¶„Ç£„É≥„Éâ„Ç¶„ÅßÂá¶ÁêÜ\n",
    "                .alias(f\"last_true_datetime_{target_category}\")\n",
    "                for target_category in target_categories\n",
    "            ]\n",
    "        )\n",
    "        .with_columns(\n",
    "            [\n",
    "                pl.when(pl.col(f\"last_true_datetime_{target_category}\").is_not_null())\n",
    "                .then(\n",
    "                    (pl.col(\"session_datetime\") - pl.col(f\"last_true_datetime_{target_category}\")) / pl.duration(days=1)\n",
    "                )\n",
    "                .otherwise(None)\n",
    "                .alias(f\"{prefix}_{target_category}\")\n",
    "                for target_category in target_categories\n",
    "            ]\n",
    "        )\n",
    "        .select(\n",
    "            pl.col(\"session_id\"),\n",
    "            *[pl.col(f\"{prefix}_{target_category}\") for target_category in target_categories],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return result_df.collect()\n",
    "\n",
    "\n",
    "@cache(cache_dir=CACHE_DIR, overwrite=not USE_FE_CACHE)\n",
    "def create_window_agg_cat_df(\n",
    "    session_df: pl.DataFrame,\n",
    "    train_log_df: pl.DataFrame,\n",
    "    jan_df: pl.DataFrame,\n",
    "    target_categories: list[str] = [\"„ÉÅ„Éß„Ç≥„É¨„Éº„Éà\", \"„Éì„Éº„É´\", \"Á±≥Ôºà5„éè‰ª•‰∏ãÔºâ\", \"„Éò„Ç¢„Ç±„Ç¢\"],  # noqa\n",
    "    window_size: str | int = \"3mo\",\n",
    "    agg_method: str = \"mean\",  # [mean, sum, max, diff, shift]\n",
    "    value_type: str = \"Â£≤‰∏äÊúâÁÑ°\",  # [Â£≤‰∏äÊúâÁÑ°, Â£≤‰∏äÊï∞Èáè, Â£≤‰∏äÈáëÈ°ç, ÂÄ§Ââ≤ÈáëÈ°ç, ÂÄ§Ââ≤Êï∞Èáè]\n",
    "    cat_type: str = \"„Ç´„ÉÜ„Ç¥„É™Âêç\",  # [„Ç´„ÉÜ„Ç¥„É™Âêç, ÈÉ®ÈñÄ]\n",
    "    prefix: str = \"window_agg_cat_\",\n",
    "    group_by: str | list[str] = \"È°ßÂÆ¢CD\",\n",
    ") -> pl.DataFrame:\n",
    "    group_by = [group_by] if isinstance(group_by, str) else group_by\n",
    "    jan_cols = [\"JAN\"] if cat_type == \"JAN\" else [\"JAN\", cat_type]\n",
    "    base_df = (\n",
    "        session_df.lazy()\n",
    "        .join(train_log_df.lazy().with_columns(pl.lit(1).alias(\"Â£≤‰∏äÊúâÁÑ°\")), on=\"session_id\", how=\"left\")\n",
    "        .join(\n",
    "            jan_df.filter(pl.col(cat_type).is_in(target_categories)).lazy().select(jan_cols),\n",
    "            on=\"JAN\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "        .select([\"session_id\", \"session_datetime\", cat_type, *group_by, value_type])\n",
    "        .unique()\n",
    "        .collect()\n",
    "        .pivot(\n",
    "            cat_type,\n",
    "            index=[*group_by, \"session_id\", \"session_datetime\"],\n",
    "            values=value_type,\n",
    "            aggregate_function=\"sum\",\n",
    "        )\n",
    "        .fill_null(0)\n",
    "    )\n",
    "    available_categories = [x for x in target_categories if x in base_df.columns]\n",
    "    base_df = base_df.select(\n",
    "        [pl.col(\"session_id\"), pl.col(\"session_datetime\")] + group_by + available_categories\n",
    "    ).lazy()\n",
    "\n",
    "    if agg_method == \"mean\":\n",
    "        agg_df = base_df.sort([*group_by, \"session_datetime\"]).select(\n",
    "            pl.col(\"session_id\"),\n",
    "            *[\n",
    "                pl.col(target)\n",
    "                .rolling_mean_by(\"session_datetime\", window_size=window_size, closed=\"left\")\n",
    "                .over(group_by)\n",
    "                .alias(f\"{prefix}_mean_{target}_{value_type}_{window_size}\")\n",
    "                for target in available_categories\n",
    "            ],\n",
    "        )\n",
    "    elif agg_method == \"sum\":\n",
    "        agg_df = base_df.sort([*group_by, \"session_datetime\"]).select(\n",
    "            pl.col(\"session_id\"),\n",
    "            *[\n",
    "                pl.col(target)\n",
    "                .rolling_sum_by(\"session_datetime\", window_size=window_size, closed=\"left\")\n",
    "                .over(group_by)\n",
    "                .alias(f\"{prefix}_sum_{target}_{value_type}_{window_size}\")\n",
    "                for target in available_categories\n",
    "            ],\n",
    "        )\n",
    "    elif agg_method == \"max\":\n",
    "        agg_df = base_df.sort([*group_by, \"session_datetime\"]).select(\n",
    "            pl.col(\"session_id\"),\n",
    "            *[\n",
    "                pl.col(target)\n",
    "                .rolling_max_by(\"session_datetime\", window_size=window_size, closed=\"left\")\n",
    "                .over(group_by)\n",
    "                .alias(f\"{prefix}_max_{target}_{value_type}_{window_size}\")\n",
    "                for target in available_categories\n",
    "            ],\n",
    "        )\n",
    "    elif agg_method == \"min\":\n",
    "        agg_df = base_df.sort([*group_by, \"session_datetime\"]).select(\n",
    "            pl.col(\"session_id\"),\n",
    "            *[\n",
    "                pl.col(target)\n",
    "                .rolling_min_by(\"session_datetime\", window_size=window_size, closed=\"left\")\n",
    "                .over(group_by)\n",
    "                .alias(f\"{prefix}_min_{target}_{value_type}_{window_size}\")\n",
    "                for target in available_categories\n",
    "            ],\n",
    "        )\n",
    "    elif agg_method == \"std\":\n",
    "        agg_df = base_df.sort([*group_by, \"session_datetime\"]).select(\n",
    "            pl.col(\"session_id\"),\n",
    "            *[\n",
    "                pl.col(target)\n",
    "                .rolling_std_by(\"session_datetime\", window_size=window_size, closed=\"left\")\n",
    "                .over(group_by)\n",
    "                .alias(f\"{prefix}_std_{target}_{value_type}_{window_size}\")\n",
    "                for target in available_categories\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    elif agg_method == \"diff\":\n",
    "        assert isinstance(window_size, int), \"window_size must be int\"\n",
    "        agg_df = base_df.sort([*group_by, \"session_datetime\"]).select(\n",
    "            pl.col(\"session_id\"),\n",
    "            *[\n",
    "                pl.col(target)\n",
    "                .diff(n=window_size)\n",
    "                .over(group_by)\n",
    "                .alias(f\"{prefix}_diff_{target}_{value_type}_{window_size}\")\n",
    "                for target in available_categories\n",
    "            ],\n",
    "        )\n",
    "    elif agg_method == \"shift\":\n",
    "        assert isinstance(window_size, int), \"window_size must be int\"\n",
    "        agg_df = base_df.sort([*group_by, \"session_datetime\"]).select(\n",
    "            pl.col(\"session_id\"),\n",
    "            *[\n",
    "                pl.col(target)\n",
    "                .shift(n=window_size)\n",
    "                .over(group_by)\n",
    "                .alias(f\"{prefix}_shift_{target}_{value_type}_{window_size}\")\n",
    "                for target in available_categories\n",
    "            ],\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid agg_method: {agg_method}\")\n",
    "\n",
    "    return agg_df.collect()\n",
    "\n",
    "\n",
    "@cache(cache_dir=CACHE_DIR, overwrite=not USE_FE_CACHE)\n",
    "def create_session_embedding_df(\n",
    "    train_log_df: pl.DataFrame,\n",
    "    session_df: pl.DataFrame,\n",
    "    jan_df: pl.DataFrame,\n",
    "    item_type: str = \"„Ç´„ÉÜ„Ç¥„É™Âêç\",\n",
    "    value_type: str = \"Â£≤‰∏äÊï∞Èáè\",\n",
    "    dim: int = 32,\n",
    "    window_size: str | int = \"6mo\",\n",
    "    agg_method: str = \"sum\",\n",
    "    group_by: str | list[str] = \"È°ßÂÆ¢CD\",\n",
    "    prefix: str = \"session_embedding_\",\n",
    ") -> pl.DataFrame:\n",
    "    print(\"üöÄ create_session_embedding_df\")\n",
    "    noleak_mtx_df = (\n",
    "        create_window_agg_cat_df(\n",
    "            session_df=session_df,\n",
    "            train_log_df=train_log_df,\n",
    "            jan_df=jan_df,\n",
    "            target_categories=jan_df[item_type].unique().to_list(),\n",
    "            window_size=window_size,\n",
    "            agg_method=agg_method,\n",
    "            cat_type=item_type,\n",
    "            value_type=value_type,\n",
    "            group_by=group_by,\n",
    "            prefix=\"\",\n",
    "        )\n",
    "        .fill_null(0)\n",
    "        .select(pl.all().shrink_dtype())\n",
    "    )\n",
    "\n",
    "    print(f\"TruncatedSVD: dim={dim}\")\n",
    "    svd = TruncatedSVD(n_components=dim, random_state=42)\n",
    "    session_embeddings = svd.fit_transform(sp.csr_matrix(noleak_mtx_df.drop(\"session_id\").to_numpy()))\n",
    "    embedding_df = pl.DataFrame(\n",
    "        session_embeddings,\n",
    "        schema=[\n",
    "            f\"{prefix}_{item_type}_{value_type}_{window_size}_{agg_method}_d{dim}_{i + 1:03}\"\n",
    "            for i in range(session_embeddings.shape[1])\n",
    "        ],\n",
    "    ).with_columns(pl.Series(name=\"session_id\", values=noleak_mtx_df[\"session_id\"]))\n",
    "    return embedding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_duration_last_cat_df = create_session_duration_last_cat_df(\n",
    "    train_log_df=train_log_df,\n",
    "    jan_df=jan_df,\n",
    "    session_df=full_session_df,\n",
    "    target_categories=get_top_co_categories(\n",
    "        train_log_df,\n",
    "        jan_df,\n",
    "        n_top_per_target=128,\n",
    "        target_categories=[\"„ÉÅ„Éß„Ç≥„É¨„Éº„Éà\", \"„Éì„Éº„É´\", \"Á±≥Ôºà5„éè‰ª•‰∏ãÔºâ\", \"„Éò„Ç¢„Ç±„Ç¢\"],\n",
    "    ),\n",
    "    cat_type=\"„Ç´„ÉÜ„Ç¥„É™Âêç\",\n",
    ")\n",
    "\n",
    "master_session_embedding_df = full_session_df.select(\"session_id\")\n",
    "for window_size, agg_method, value_type in tqdm(\n",
    "    [\n",
    "        # window_size, agg_method, value_type\n",
    "        (\"6mo\", \"sum\", \"Â£≤‰∏äÊï∞Èáè\"),\n",
    "        # (\"1mo\", \"sum\", \"Â£≤‰∏äÊï∞Èáè\"),\n",
    "        (\"1w\", \"sum\", \"Â£≤‰∏äÊï∞Èáè\"),\n",
    "        (\"3d\", \"sum\", \"Â£≤‰∏äÊï∞Èáè\"),\n",
    "        # (\"6mo\", \"max\", \"Â£≤‰∏äÊï∞Èáè\"),\n",
    "        # (\"1mo\", \"max\", \"Â£≤‰∏äÊï∞Èáè\"),\n",
    "        # (\"1w\", \"max\", \"Â£≤‰∏äÊï∞Èáè\"),\n",
    "        # (\"3d\", \"max\", \"Â£≤‰∏äÊï∞Èáè\"),\n",
    "        # (\"6mo\", \"min\", \"Â£≤‰∏äÊï∞Èáè\"),\n",
    "        # (\"1mo\", \"min\", \"Â£≤‰∏äÊï∞Èáè\"),\n",
    "        # (\"1w\", \"min\", \"Â£≤‰∏äÊï∞Èáè\"),\n",
    "        # (\"3d\", \"min\", \"Â£≤‰∏äÊï∞Èáè\"),\n",
    "    ]\n",
    "):\n",
    "    master_session_embedding_df = master_session_embedding_df.join(\n",
    "        create_session_embedding_df(\n",
    "            train_log_df=train_log_df,\n",
    "            session_df=full_session_df,\n",
    "            jan_df=jan_df,\n",
    "            item_type=\"„Ç´„ÉÜ„Ç¥„É™Âêç\",\n",
    "            value_type=value_type,\n",
    "            dim=128,\n",
    "            window_size=window_size,\n",
    "            agg_method=agg_method,\n",
    "            prefix=\"session_embedding_v1_\",\n",
    "        ),\n",
    "        on=\"session_id\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    master_session_embedding_df = master_session_embedding_df.join(\n",
    "        create_session_embedding_df(\n",
    "            train_log_df=train_log_df,\n",
    "            session_df=full_session_df,\n",
    "            jan_df=jan_df,\n",
    "            item_type=\"„Ç´„ÉÜ„Ç¥„É™Âêç\",\n",
    "            value_type=value_type,\n",
    "            dim=128,\n",
    "            window_size=window_size,\n",
    "            agg_method=agg_method,\n",
    "            group_by=[\"Âπ¥‰ª£\", \"ÊÄßÂà•\", \"Â∫óËàóÂêç\"],\n",
    "            prefix=\"session_embedding_v2_\",\n",
    "        ),\n",
    "        on=\"session_id\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "master_rolling_agg_cat_df = full_session_df.select(\"session_id\")\n",
    "for window_size, agg_method, value_type in tqdm(\n",
    "    [\n",
    "        # window_size, agg_method, value_type\n",
    "        (\"6mo\", \"sum\", \"Â£≤‰∏äÊï∞Èáè\"),\n",
    "        # (\"1mo\", \"sum\", \"Â£≤‰∏äÊï∞Èáè\"),\n",
    "        (\"1w\", \"sum\", \"Â£≤‰∏äÊï∞Èáè\"),\n",
    "        (\"3d\", \"sum\", \"Â£≤‰∏äÊï∞Èáè\"),\n",
    "        (\"6mo\", \"mean\", \"Â£≤‰∏äÊï∞Èáè\"),\n",
    "        # (\"1mo\", \"mean\", \"Â£≤‰∏äÊï∞Èáè\"),\n",
    "        (\"1w\", \"mean\", \"Â£≤‰∏äÊï∞Èáè\"),\n",
    "        (\"3d\", \"mean\", \"Â£≤‰∏äÊï∞Èáè\"),\n",
    "        (\"6mo\", \"max\", \"Â£≤‰∏äÊï∞Èáè\"),\n",
    "        # (\"1mo\", \"max\", \"Â£≤‰∏äÊï∞Èáè\"),\n",
    "        (\"1w\", \"max\", \"Â£≤‰∏äÊï∞Èáè\"),\n",
    "        (\"3d\", \"max\", \"Â£≤‰∏äÊï∞Èáè\"),\n",
    "        (\"6mo\", \"std\", \"Â£≤‰∏äÊï∞Èáè\"),\n",
    "        # (\"1mo\", \"std\", \"Â£≤‰∏äÊï∞Èáè\"),\n",
    "        (\"1w\", \"std\", \"Â£≤‰∏äÊï∞Èáè\"),\n",
    "        (\"3d\", \"std\", \"Â£≤‰∏äÊï∞Èáè\"),\n",
    "        (\"6mo\", \"max\", \"ÂÄ§Ââ≤Êï∞Èáè\"),\n",
    "        (\"6mo\", \"sum\", \"ÂÄ§Ââ≤Êï∞Èáè\"),\n",
    "        (\"6mo\", \"mean\", \"ÂÄ§Ââ≤Êï∞Èáè\"),\n",
    "        (\"6mo\", \"std\", \"ÂÄ§Ââ≤Êï∞Èáè\"),\n",
    "    ]\n",
    "):\n",
    "    rolling_agg_df = create_window_agg_cat_df(\n",
    "        session_df=full_session_df,\n",
    "        train_log_df=train_log_df,\n",
    "        jan_df=jan_df,\n",
    "        target_categories=get_top_co_categories(\n",
    "            train_log_df,\n",
    "            jan_df,\n",
    "            n_top_per_target=16,\n",
    "            target_categories=[\"„ÉÅ„Éß„Ç≥„É¨„Éº„Éà\", \"„Éì„Éº„É´\", \"Á±≥Ôºà5„éè‰ª•‰∏ãÔºâ\", \"„Éò„Ç¢„Ç±„Ç¢\"],\n",
    "        ),\n",
    "        window_size=window_size,\n",
    "        agg_method=agg_method,\n",
    "        value_type=value_type,\n",
    "        cat_type=\"„Ç´„ÉÜ„Ç¥„É™Âêç\",\n",
    "    )\n",
    "\n",
    "    master_rolling_agg_cat_df = master_rolling_agg_cat_df.join(\n",
    "        rolling_agg_df,\n",
    "        on=\"session_id\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "\n",
    "# add feature\n",
    "full_session_df_with_feats = (\n",
    "    (\n",
    "        full_session_df.with_columns(pl.lit(1).alias(\"dummy\"))\n",
    "        .with_columns(\n",
    "            pl.col(\"È°ßÂÆ¢CD\")\n",
    "            .is_in(full_session_df.filter(pl.col(\"dataset\") != \"TRAIN\")[\"È°ßÂÆ¢CD\"].unique())\n",
    "            .alias(\"is_hot\"),\n",
    "            pl.col(\"Â£≤‰∏äÊó•\").dt.day().alias(\"day\"),\n",
    "            pl.col(\"Â£≤‰∏äÊó•\").dt.weekday().alias(\"weekday\"),\n",
    "            pl.col(\"Âπ¥‰ª£\")\n",
    "            .replace(\n",
    "                {\n",
    "                    \"10‰ª£‰ª•‰∏ã\": 10,\n",
    "                    \"20‰ª£\": 20,\n",
    "                    \"30‰ª£\": 30,\n",
    "                    \"40‰ª£\": 40,\n",
    "                    \"50‰ª£\": 50,\n",
    "                    \"60‰ª£\": 60,\n",
    "                    \"70‰ª£\": 70,\n",
    "                    \"80‰ª£‰ª•‰∏ä\": 80,\n",
    "                    \"‰∏çÊòé\": None,\n",
    "                }\n",
    "            )\n",
    "            .cast(pl.Float32)\n",
    "            .alias(\"age\"),\n",
    "            pl.col(\"session_id\").cum_count().over(\"È°ßÂÆ¢CD\").alias(\"cum_visit_count\"),\n",
    "            pl.col(\"session_datetime\").diff().over(\"È°ßÂÆ¢CD\").alias(\"days_since_last_visit\").dt.total_days(),\n",
    "            pl.col(\"dummy\")\n",
    "            .rolling_sum_by(\"session_datetime\", window_size=\"1mo\")\n",
    "            .over(\"È°ßÂÆ¢CD\")\n",
    "            .alias(\"visit_count_1mo\"),\n",
    "            pl.col(\"dummy\").rolling_sum_by(\"session_datetime\", window_size=\"1w\").over(\"È°ßÂÆ¢CD\").alias(\"visit_count_1w\"),\n",
    "            pl.col(\"dummy\").sum().over([\"È°ßÂÆ¢CD\", \"Â£≤‰∏äÊó•\"]).alias(\"visit_count_today\"),\n",
    "            pl.col(\"dummy\").sum().over([\"È°ßÂÆ¢CD\"]).alias(\"visit_count_total\"),\n",
    "            pl.when(pl.col(\"ÊôÇÂàª\").is_between(8, 12, closed=\"both\")).then(0).otherwise(1).alias(\"is_am\"),\n",
    "        )\n",
    "        .drop(\"dummy\")\n",
    "    )\n",
    "    .join(holiday_df, on=\"Â£≤‰∏äÊó•\", how=\"left\")\n",
    "    .with_columns(pl.col(\"is_holiday\").fill_null(0))\n",
    "    .join(session_duration_last_cat_df, on=\"session_id\", how=\"left\")\n",
    "    .join(master_rolling_agg_cat_df, on=\"session_id\", how=\"left\")\n",
    "    .join(master_session_embedding_df, on=\"session_id\", how=\"left\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fe(\n",
    "    train_df: pl.DataFrame,\n",
    "    test_df: pl.DataFrame,\n",
    "    valid_df: pl.DataFrame | None = None,\n",
    ") -> tuple[pl.DataFrame, pl.DataFrame]:\n",
    "    encoders = [\n",
    "        RawEncoder(\n",
    "            columns=META_COLS,\n",
    "            prefix=\"\",\n",
    "        ),\n",
    "        RawEncoder(\n",
    "            columns=[\n",
    "                \"ÊôÇÂàª\",\n",
    "                \"day\",\n",
    "                \"weekday\",\n",
    "                \"age\",\n",
    "                \"cum_visit_count\",\n",
    "                \"days_since_last_visit\",\n",
    "                \"visit_count_1mo\",\n",
    "                \"visit_count_1w\",\n",
    "                \"visit_count_today\",\n",
    "                \"visit_count_total\",\n",
    "                \"is_hot\",\n",
    "                \"is_holiday\",\n",
    "                # \"is_am\",\n",
    "                *[x for x in train_df.columns if x.startswith(\"days_since_cat_\")],\n",
    "                *[x for x in train_df.columns if x.startswith(\"window_agg_cat_\")],\n",
    "                *[x for x in train_df.columns if x.startswith(\"session_embedding_\")],\n",
    "            ],\n",
    "            prefix=NUMERICAL_FEATURE_PREFIX,\n",
    "        ),\n",
    "        OrdinalEncoder(\n",
    "            columns=[\n",
    "                \"ÊÄßÂà•\",\n",
    "                \"È°ßÂÆ¢CD\",\n",
    "                \"Â∫óËàóÂêç\",\n",
    "            ],\n",
    "            prefix=CATEGORICAL_FEATURE_PREFIX,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # train, test „Å´ transform\n",
    "    train_feature_df = pl.concat(\n",
    "        [encoder.fit_transform(train_df) for encoder in encoders],\n",
    "        how=\"horizontal\",\n",
    "    ).select(pl.all().shrink_dtype())\n",
    "\n",
    "    test_feature_df = pl.concat(\n",
    "        [encoder.transform(test_df) for encoder in encoders],\n",
    "        how=\"horizontal\",\n",
    "    ).select(pl.all().shrink_dtype())\n",
    "\n",
    "    if valid_df is not None:\n",
    "        valid_feature_df = pl.concat(\n",
    "            [encoder.transform(valid_df) for encoder in encoders],\n",
    "            how=\"horizontal\",\n",
    "        ).select(pl.all().shrink_dtype())\n",
    "        return train_feature_df, test_feature_df, valid_feature_df\n",
    "\n",
    "    return train_feature_df, test_feature_df\n",
    "\n",
    "\n",
    "train_df = full_session_df_with_feats.filter(pl.col(\"dataset\") == \"TRAIN\")\n",
    "valid_df = full_session_df_with_feats.filter(pl.col(\"dataset\") == \"VALID\")\n",
    "test_df = full_session_df_with_feats.filter(pl.col(\"dataset\") == \"TEST\")\n",
    "\n",
    "train_feature_df, test_feature_df, valid_feature_df = fe(train_df, test_df, valid_df=valid_df)\n",
    "\n",
    "cat_feature_cols = [x for x in train_feature_df.columns if x.startswith(CATEGORICAL_FEATURE_PREFIX)]\n",
    "num_feature_cols = [x for x in train_feature_df.columns if x.startswith(NUMERICAL_FEATURE_PREFIX)]\n",
    "feature_cols = cat_feature_cols + num_feature_cols\n",
    "\n",
    "print(f\"numerical features: {len(num_feature_cols)}\")\n",
    "print(f\"categorical features: {len(cat_feature_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalFn:\n",
    "    def __init__(self, target_col: str, pred_col: str = \"pred\"):\n",
    "        self.target_col = target_col\n",
    "\n",
    "    def __call__(self, input_df: pl.DataFrame) -> dict[str, float]:\n",
    "        y_true = input_df[self.target_col].to_numpy()\n",
    "        y_pred = input_df[\"pred\"].to_numpy()\n",
    "\n",
    "        scores = {\n",
    "            \"rocauc\": roc_auc_score(y_true, y_pred),\n",
    "        }\n",
    "        return scores\n",
    "\n",
    "    @property\n",
    "    def __name__(self) -> str:\n",
    "        return self.__class__.__name__\n",
    "\n",
    "\n",
    "def add_kfold(\n",
    "    input_df: pl.DataFrame,\n",
    "    n_splits: int,\n",
    "    random_state: int,\n",
    "    fold_col: str,\n",
    ") -> pl.DataFrame:\n",
    "    if n_splits == 1:\n",
    "        return input_df.with_columns(pl.lit(0).alias(fold_col))\n",
    "\n",
    "    skf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)  # NOTE:gkf „Å´„Åô„Çã„Åπ„Åç?\n",
    "    folds = np.zeros(len(input_df), dtype=np.int32)\n",
    "    for fold, (_, valid_idx) in enumerate(skf.split(X=input_df)):\n",
    "        folds[valid_idx] = fold\n",
    "    return input_df.with_columns(pl.Series(name=fold_col, values=folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, scores = {}, {}\n",
    "\n",
    "for target_col in TARGET_COLS:\n",
    "    va_result_df, va_scores = pl.DataFrame(), {}\n",
    "    all_models = []\n",
    "    for seed in SEEDS:\n",
    "        print(f\"# ================== seed={seed}: {target_col} ================== #\")\n",
    "        name = f\"seed{seed}_lgb_{target_col}\"\n",
    "        _va_result_df, _va_scores, trained_models = single_train_fn(\n",
    "            model=LightGBMWapper(\n",
    "                name=name,\n",
    "                model=lgb.LGBMModel(\n",
    "                    objective=\"binary\",\n",
    "                    boosting=\"gbdt\",\n",
    "                    n_estimators=10000,\n",
    "                    learning_rate=0.01,\n",
    "                    num_leaves=31,\n",
    "                    colsample_bytree=0.1,\n",
    "                    subsample=0.1,\n",
    "                    importance_type=\"gain\",\n",
    "                    random_state=seed,\n",
    "                    force_col_wise=True,\n",
    "                    class_weight=\"balanced\",\n",
    "                    # verbose=-1,\n",
    "                ),\n",
    "                fit_params={\n",
    "                    \"callbacks\": [\n",
    "                        lgb.early_stopping(100, first_metric_only=True),\n",
    "                        lgb.log_evaluation(period=100),\n",
    "                    ],\n",
    "                    \"categorical_feature\": cat_feature_cols,\n",
    "                    \"feature_name\": feature_cols,\n",
    "                    \"eval_metric\": \"auc\",\n",
    "                },\n",
    "            ),\n",
    "            features_df=add_kfold(\n",
    "                train_feature_df,\n",
    "                n_splits=N_SPLITS,\n",
    "                random_state=seed,\n",
    "                fold_col=FOLD_COL,\n",
    "            ),\n",
    "            feature_cols=feature_cols,\n",
    "            target_col=target_col,\n",
    "            fold_col=FOLD_COL,\n",
    "            meta_cols=META_COLS + [FOLD_COL],\n",
    "            out_dir=EXP_OUTPUT_DIR,\n",
    "            eval_fn=EvalFn(target_col=target_col),\n",
    "            overwrite=True,\n",
    "            val_features_df=valid_feature_df,\n",
    "            full_training=True,\n",
    "        )\n",
    "        va_result_df = pl.concat([va_result_df, _va_result_df], how=\"diagonal_relaxed\")\n",
    "        va_scores[name] = _va_scores\n",
    "        all_models.extend(trained_models)\n",
    "\n",
    "    va_result_agg_df = (\n",
    "        va_result_df.group_by(\"session_id\")\n",
    "        .agg(pl.col(\"pred\").mean())\n",
    "        .sort(\"session_id\")\n",
    "        .join(va_result_df.select(META_COLS), on=\"session_id\", how=\"left\")\n",
    "    )\n",
    "    results[target_col] = {\n",
    "        \"result_df\": va_result_agg_df,\n",
    "        \"models\": all_models,\n",
    "    }\n",
    "    scores[target_col] = va_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_va_result_df(results: dict) -> pl.DataFrame:\n",
    "    result_dfs = {k: x[\"result_df\"].with_columns(pl.col(\"pred\").alias(f\"pred_{k}\")) for k, x in results.items()}\n",
    "\n",
    "    va_result_df = pl.DataFrame()\n",
    "    for i, (name, result_df) in enumerate(result_dfs.items()):\n",
    "        if i == 0:\n",
    "            i_df = result_df.select([\"session_id\", \"È°ßÂÆ¢CD\", f\"pred_{name}\", name])\n",
    "        else:\n",
    "            i_df = result_df.select([f\"pred_{name}\", name])\n",
    "        va_result_df = pl.concat([va_result_df, i_df], how=\"horizontal\")\n",
    "    return va_result_df\n",
    "\n",
    "\n",
    "va_result_df = construct_va_result_df(results)\n",
    "pred_cols = [f\"pred_{x}\" for x in TARGET_COLS]\n",
    "score = roc_auc_score(va_result_df[TARGET_COLS].to_numpy(), va_result_df[pred_cols].to_numpy(), average=\"macro\")\n",
    "scores[\"final_metric\"] = score\n",
    "\n",
    "va_result_df.write_parquet(EXP_OUTPUT_DIR / \"va_result_df.parquet\")\n",
    "with open(EXP_OUTPUT_DIR / \"scores.json\", \"w\") as f:\n",
    "    json.dump(scores, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(json.dumps(scores, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_result_df = pl.DataFrame()\n",
    "for i, (name, res) in enumerate(results.items()):\n",
    "    target_name = name\n",
    "    if i == 0:\n",
    "        cols = [pl.col(\"session_id\"), pl.col(\"pred\").alias(target_name)]\n",
    "    else:\n",
    "        cols = [pl.col(\"pred\").alias(target_name)]\n",
    "\n",
    "    _te_result_df = single_inference_fn_v2(\n",
    "        models=res[\"models\"],\n",
    "        features_df=test_feature_df,\n",
    "        feature_names=feature_cols,\n",
    "    ).select(cols)\n",
    "    te_result_df = pl.concat([te_result_df, _te_result_df], how=\"horizontal\")\n",
    "\n",
    "submission_df = (\n",
    "    test_session_df.select(\"session_id\")\n",
    "    .join(te_result_df, on=\"session_id\", how=\"left\")\n",
    "    .select([\"session_id\"] + TARGET_COLS)\n",
    ")\n",
    "\n",
    "submission_df.write_parquet(EXP_OUTPUT_DIR / \"te_result_df.parquet\")\n",
    "submission_df.select(\n",
    "    [\n",
    "        \"„ÉÅ„Éß„Ç≥„É¨„Éº„Éà\",\n",
    "        \"„Éì„Éº„É´\",\n",
    "        \"„Éò„Ç¢„Ç±„Ç¢\",\n",
    "        \"Á±≥Ôºà5„éè‰ª•‰∏ãÔºâ\",\n",
    "    ]\n",
    ").write_csv(EXP_OUTPUT_DIR / \"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
