{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import rootutils\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "ROOT = rootutils.setup_root(\".\", pythonpath=True, cwd=True)\n",
    "\n",
    "from src.feature.tabular import OrdinalEncoder, RawEncoder\n",
    "from src.feature.utils import cache\n",
    "from src.model.sklearn_like import LightGBMWapper\n",
    "from src.trainer.tabular.simple import single_inference_fn_v2, single_train_fn\n",
    "\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "INPUT_DIR = DATA_DIR / \"atmacup19_dataset\"\n",
    "OUTPUT_DIR = DATA_DIR / \"output\"\n",
    "CACHE_DIR = DATA_DIR / \"cache\"\n",
    "\n",
    "for d in [DATA_DIR, INPUT_DIR, OUTPUT_DIR, CACHE_DIR]:\n",
    "    d.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "pl.Config.set_fmt_str_lengths(200)\n",
    "pl.Config.set_tbl_cols(50)\n",
    "pl.Config.set_tbl_rows(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"006\"\n",
    "\n",
    "SEED = 42\n",
    "N_SPLITS = 3\n",
    "\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_OUTPUT_DIR = OUTPUT_DIR / EXP_NAME\n",
    "EXP_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FEATURE_PREFIX = \"f_\"\n",
    "NUMERICAL_FEATURE_PREFIX = f\"{FEATURE_PREFIX}n_\"\n",
    "CATEGORICAL_FEATURE_PREFIX = f\"{FEATURE_PREFIX}c_\"\n",
    "\n",
    "TARGET_COLS = [\n",
    "    \"ビール\",\n",
    "    \"ヘアケア\",\n",
    "    \"チョコレート\",\n",
    "    \"米（5㎏以下）\",\n",
    "]\n",
    "META_COLS = [\"session_id\", \"顧客CD\"] + TARGET_COLS\n",
    "FOLD_COL = \"fold\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec_log_df = pl.read_csv(INPUT_DIR / \"ec_log.csv\", infer_schema_length=200000)\n",
    "jan_df = pl.read_csv(INPUT_DIR / \"jan.csv\")\n",
    "test_session_df = pl.read_csv(INPUT_DIR / \"test_session.csv\")\n",
    "train_session_df = pl.concat(\n",
    "    [pl.read_csv(INPUT_DIR / \"train_session.csv\"), pl.read_csv(INPUT_DIR / \"train_target.csv\")], how=\"horizontal\"\n",
    ")\n",
    "\n",
    "# sampling\n",
    "if DEBUG:\n",
    "    train_session_df = train_session_df.sample(10000, seed=SEED)\n",
    "\n",
    "train_log_df = pl.read_csv(INPUT_DIR / \"train_log.csv\")\n",
    "\n",
    "raw_full_session_df = (\n",
    "    pl.concat(\n",
    "        [\n",
    "            train_session_df.with_columns(dataset=pl.lit(\"TRAIN\")),\n",
    "            test_session_df.with_columns(dataset=pl.lit(\"TEST\")),\n",
    "        ],\n",
    "        how=\"diagonal_relaxed\",\n",
    "    )\n",
    "    .with_columns(pl.col(\"売上日\").cast(pl.Date))\n",
    "    .with_columns(\n",
    "        pl.datetime(\n",
    "            pl.col(\"売上日\").dt.year(), pl.col(\"売上日\").dt.month(), pl.col(\"売上日\").dt.day(), pl.col(\"時刻\")\n",
    "        ).alias(\"session_datetime\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"train_session_df: {train_session_df.shape}\")\n",
    "print(f\"test_session_df: {test_session_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_lists(list_a, list_b):\n",
    "    set_a = set(list_a)\n",
    "    set_b = set(list_b)\n",
    "\n",
    "    common = set_a & set_b\n",
    "    only_a = set_a - set_b\n",
    "    only_b = set_b - set_a\n",
    "\n",
    "    return (list(common), list(only_a), list(only_b))\n",
    "\n",
    "\n",
    "common_user_ids, only_train_user_ids, only_test_user_ids = compare_lists(\n",
    "    list_a=train_session_df[\"顧客CD\"].unique().to_numpy(),\n",
    "    list_b=test_session_df[\"顧客CD\"].unique().to_numpy(),\n",
    ")\n",
    "print(len(common_user_ids), len(only_train_user_ids), len(only_test_user_ids))\n",
    "\n",
    "# common 以外の user id を null にする\n",
    "full_session_df = raw_full_session_df.with_columns(\n",
    "    pl.when(pl.col(\"顧客CD\").is_in(common_user_ids))\n",
    "    .then(pl.col(\"顧客CD\"))\n",
    "    .otherwise(pl.lit(\"__NULL__\"))\n",
    "    .alias(\"顧客CD\"),\n",
    "    pl.col(\"顧客CD\").alias(\"original_顧客CD\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @cache(cache_dir=CACHE_DIR, overwrite=False)\n",
    "def create_amount_pivot_df(\n",
    "    train_log_df: pl.DataFrame,\n",
    "    jan_df: pl.DataFrame,\n",
    "    jan_type: str = \"部門\",\n",
    "    amount_types: tuple[str] | None = None,\n",
    "    prefix: str = \"\",\n",
    "    agg_method: str = \"sum\",\n",
    ") -> pl.DataFrame:\n",
    "    # session id ごとに JAN と {jan_type} を紐づける\n",
    "    amount_types = amount_types or [\"売上数量\", \"売上金額\", \"値割金額\", \"値割数量\"]\n",
    "\n",
    "    agg_exprs = []\n",
    "    for amount_type in amount_types:\n",
    "        if agg_method == \"sum\":\n",
    "            agg_exprs.append(pl.col(amount_type).sum())\n",
    "        elif agg_method == \"mean\":\n",
    "            agg_exprs.append(pl.col(amount_type).mean())\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid agg_method: {agg_method}\")\n",
    "\n",
    "    session_agg_jan_df = (\n",
    "        train_log_df.lazy()\n",
    "        .join(jan_df.lazy(), on=\"JAN\", how=\"inner\")\n",
    "        .select(\n",
    "            pl.col(\"session_id\"),\n",
    "            *[pl.col(amount_type) for amount_type in amount_types],\n",
    "            pl.col(jan_type),\n",
    "        )\n",
    "        .group_by([\"session_id\", jan_type])\n",
    "        .agg(agg_exprs)\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    # session_id を index, jan_type における amount_types を column に持つ pivot table を作成\n",
    "    amount_pivot_df = pl.DataFrame()\n",
    "    for i, amount_type in enumerate(amount_types):\n",
    "        cols = [pl.exclude(\"session_id\").name.prefix(f\"{prefix}{jan_type}_{amount_type}_{agg_method}_\")]\n",
    "        if i == 0:\n",
    "            cols.insert(0, pl.col(\"session_id\"))\n",
    "\n",
    "        amount_pivot_df = pl.concat(\n",
    "            [\n",
    "                amount_pivot_df,\n",
    "                session_agg_jan_df.pivot(jan_type, index=\"session_id\", values=amount_type)\n",
    "                .sort(\"session_id\")\n",
    "                .select(*cols),\n",
    "            ],\n",
    "            how=\"horizontal\",\n",
    "        )\n",
    "\n",
    "    return amount_pivot_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user\n",
    "amount_pivot_df = create_amount_pivot_df(\n",
    "    train_log_df=train_log_df,\n",
    "    jan_df=jan_df.filter(\n",
    "        pl.col(\"カテゴリ名\").is_in(\n",
    "            [\n",
    "                \"ビール\",\n",
    "                \"ヘアケア\",\n",
    "                \"チョコレート\",\n",
    "                \"米（5㎏以下）\",\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "    jan_type=\"カテゴリ名\",\n",
    "    amount_types=(\"売上数量\",),\n",
    "    prefix=\"amount_\",\n",
    ")\n",
    "\n",
    "\n",
    "for group_key in [\n",
    "    [\"顧客CD\"],\n",
    "    [\"店舗名\", \"年代\", \"性別\"],\n",
    "    # [\"店舗名\", \"年代\", \"性別\", \"時刻\"],\n",
    "]:\n",
    "    group_key_str = \"_\".join(group_key)\n",
    "    user_amount_pivot_df = (\n",
    "        full_session_df.filter(pl.col(\"dataset\") == \"TRAIN\")\n",
    "        .select([\"session_id\"] + group_key)\n",
    "        .join(amount_pivot_df, on=\"session_id\", how=\"inner\")\n",
    "        .drop(\"session_id\")\n",
    "        .group_by(group_key)\n",
    "        .agg(pl.col(\"*\").sum())\n",
    "    )\n",
    "\n",
    "    value_cols = [x for x in user_amount_pivot_df.columns if x not in group_key]\n",
    "\n",
    "    full_session_df = (\n",
    "        full_session_df.join(user_amount_pivot_df, on=group_key, how=\"left\")\n",
    "        .with_columns([pl.col(x).fill_null(0).name.suffix(f\"_grpby_{group_key_str}\") for x in value_cols])\n",
    "        .drop(value_cols)\n",
    "    )\n",
    "\n",
    "\n",
    "# add feature\n",
    "full_session_df = (\n",
    "    full_session_df.with_columns(pl.lit(1).alias(\"dummy\"))\n",
    "    .with_columns(\n",
    "        pl.col(\"顧客CD\").is_in(full_session_df.filter(pl.col(\"dataset\") != \"TRAIN\")[\"顧客CD\"].unique()).alias(\"is_hot\"),\n",
    "        pl.col(\"売上日\").dt.day().alias(\"day\"),\n",
    "        pl.col(\"売上日\").dt.weekday().alias(\"weekday\"),\n",
    "        pl.col(\"年代\")\n",
    "        .replace(\n",
    "            {\n",
    "                \"10代以下\": 10,\n",
    "                \"20代\": 20,\n",
    "                \"30代\": 30,\n",
    "                \"40代\": 40,\n",
    "                \"50代\": 50,\n",
    "                \"60代\": 60,\n",
    "                \"70代\": 70,\n",
    "                \"80代以上\": 80,\n",
    "                \"不明\": None,\n",
    "            }\n",
    "        )\n",
    "        .cast(pl.Float32)\n",
    "        .alias(\"age\"),\n",
    "        pl.col(\"session_id\").cum_count().over(\"顧客CD\").alias(\"cum_visit_count\"),\n",
    "        pl.col(\"session_datetime\").diff().over(\"顧客CD\").alias(\"days_since_last_visit\").dt.total_days(),\n",
    "        pl.col(\"dummy\").rolling_sum_by(\"session_datetime\", window_size=\"1mo\").over(\"顧客CD\").alias(\"visit_count_1mo\"),\n",
    "        pl.col(\"dummy\").rolling_sum_by(\"session_datetime\", window_size=\"1w\").over(\"顧客CD\").alias(\"visit_count_1w\"),\n",
    "        pl.col(\"dummy\").sum().over([\"顧客CD\", \"売上日\"]).alias(\"visit_count_today\"),\n",
    "        pl.col(\"dummy\").sum().over([\"顧客CD\"]).alias(\"visit_count_total\"),\n",
    "    )\n",
    "    .drop(\"dummy\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fe(train_df: pl.DataFrame, test_df: pl.DataFrame) -> tuple[pl.DataFrame, pl.DataFrame]:\n",
    "    encoders = [\n",
    "        RawEncoder(\n",
    "            columns=META_COLS,\n",
    "            prefix=\"\",\n",
    "        ),\n",
    "        RawEncoder(\n",
    "            columns=[\n",
    "                *[x for x in train_df.columns if x.startswith(\"amount_\")],\n",
    "                \"時刻\",\n",
    "                \"day\",\n",
    "                \"weekday\",\n",
    "                \"age\",\n",
    "                \"cum_visit_count\",\n",
    "                \"days_since_last_visit\",\n",
    "                \"visit_count_1mo\",\n",
    "                \"visit_count_1w\",\n",
    "                \"visit_count_today\",\n",
    "                \"visit_count_total\",\n",
    "                \"is_hot\",\n",
    "            ],\n",
    "            prefix=NUMERICAL_FEATURE_PREFIX,\n",
    "        ),\n",
    "        OrdinalEncoder(\n",
    "            columns=[\n",
    "                \"性別\",\n",
    "                \"顧客CD\",\n",
    "                \"店舗名\",\n",
    "            ],\n",
    "            prefix=CATEGORICAL_FEATURE_PREFIX,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # train のみで fit\n",
    "    for encoder in encoders:\n",
    "        print(f\"fit: {encoder}\")\n",
    "        encoder.fit(pl.concat([train_df, test_df], how=\"diagonal_relaxed\"))\n",
    "\n",
    "    # train, test に transform\n",
    "    train_feature_df = pl.concat(\n",
    "        [encoder.fit_transform(train_df) for encoder in encoders],\n",
    "        how=\"horizontal\",\n",
    "    )\n",
    "\n",
    "    test_feature_df = pl.concat(\n",
    "        [encoder.transform(test_df) for encoder in encoders],\n",
    "        how=\"horizontal\",\n",
    "    )\n",
    "    return train_feature_df, test_feature_df\n",
    "\n",
    "\n",
    "train_df = full_session_df.filter(pl.col(\"dataset\") != \"TEST\")\n",
    "test_df = full_session_df.filter(pl.col(\"dataset\") == \"TEST\")\n",
    "\n",
    "train_feature_df, test_feature_df = fe(train_df, test_df)\n",
    "\n",
    "cat_feature_cols = [x for x in train_feature_df.columns if x.startswith(CATEGORICAL_FEATURE_PREFIX)]\n",
    "num_feature_cols = [x for x in train_feature_df.columns if x.startswith(NUMERICAL_FEATURE_PREFIX)]\n",
    "feature_cols = cat_feature_cols + num_feature_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalFn:\n",
    "    def __init__(self, target_col: str, pred_col: str = \"pred\"):\n",
    "        self.target_col = target_col\n",
    "\n",
    "    def __call__(self, input_df: pl.DataFrame) -> dict[str, float]:\n",
    "        y_true = input_df[self.target_col].to_numpy()\n",
    "        y_pred = input_df[\"pred\"].to_numpy()\n",
    "\n",
    "        scores = {\n",
    "            \"rocauc\": roc_auc_score(y_true, y_pred),\n",
    "        }\n",
    "        return scores\n",
    "\n",
    "    @property\n",
    "    def __name__(self) -> str:\n",
    "        return self.__class__.__name__\n",
    "\n",
    "\n",
    "def add_kfold(\n",
    "    input_df: pl.DataFrame,\n",
    "    n_splits: int,\n",
    "    random_state: int,\n",
    "    fold_col: str,\n",
    ") -> pl.DataFrame:\n",
    "    skf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)  # NOTE:gkf にするべき?\n",
    "    folds = np.zeros(len(input_df), dtype=np.int32)\n",
    "    for fold, (_, valid_idx) in enumerate(skf.split(X=input_df)):\n",
    "        folds[valid_idx] = fold\n",
    "    return input_df.with_columns(pl.Series(name=fold_col, values=folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "scores = {}\n",
    "for target_col in TARGET_COLS:\n",
    "    name = f\"lgb_{target_col}\"\n",
    "    _va_result_df, _va_scores, trained_models = single_train_fn(\n",
    "        model=LightGBMWapper(\n",
    "            name=name,\n",
    "            model=lgb.LGBMModel(\n",
    "                objective=\"binary\",\n",
    "                boosting=\"gbdt\",\n",
    "                n_estimators=5000,\n",
    "                learning_rate=0.1,\n",
    "                num_leaves=31,\n",
    "                colsample_bytree=0.1,\n",
    "                subsample=0.1,\n",
    "                importance_type=\"gain\",\n",
    "                random_state=SEED,\n",
    "                force_col_wise=True,\n",
    "            ),\n",
    "            fit_params={\n",
    "                \"callbacks\": [\n",
    "                    lgb.early_stopping(100, first_metric_only=True),\n",
    "                    lgb.log_evaluation(period=100),\n",
    "                ],\n",
    "                \"categorical_feature\": cat_feature_cols,\n",
    "                \"feature_name\": feature_cols,\n",
    "                \"eval_metric\": \"auc\",\n",
    "            },\n",
    "        ),\n",
    "        features_df=add_kfold(\n",
    "            train_feature_df,\n",
    "            n_splits=N_SPLITS,\n",
    "            random_state=SEED,\n",
    "            fold_col=FOLD_COL,\n",
    "        ),\n",
    "        feature_cols=feature_cols,\n",
    "        target_col=target_col,\n",
    "        fold_col=FOLD_COL,\n",
    "        meta_cols=META_COLS + [FOLD_COL],\n",
    "        out_dir=EXP_OUTPUT_DIR,\n",
    "        eval_fn=EvalFn(target_col=target_col),\n",
    "        overwrite=False,\n",
    "    )\n",
    "\n",
    "    results[name] = {\n",
    "        \"result_df\": _va_result_df,\n",
    "        \"models\": trained_models,\n",
    "    }\n",
    "    scores[name] = _va_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_va_result_df(results: dict) -> pl.DataFrame:\n",
    "    result_dfs = {\n",
    "        k.split(\"_\")[1]: x[\"result_df\"].with_columns(pl.col(\"pred\").alias(f\"pred_{k.split('_')[1]}\"))\n",
    "        for k, x in results.items()\n",
    "    }\n",
    "\n",
    "    va_result_df = pl.DataFrame()\n",
    "    for i, (name, result_df) in enumerate(result_dfs.items()):\n",
    "        if i == 0:\n",
    "            i_df = result_df.select([\"session_id\", \"顧客CD\", f\"pred_{name}\", name])\n",
    "        else:\n",
    "            i_df = result_df.select([f\"pred_{name}\", name])\n",
    "        va_result_df = pl.concat([va_result_df, i_df], how=\"horizontal\")\n",
    "    return va_result_df\n",
    "\n",
    "\n",
    "va_result_df = construct_va_result_df(results)\n",
    "pred_cols = [f\"pred_{x}\" for x in TARGET_COLS]\n",
    "score = roc_auc_score(va_result_df[TARGET_COLS].to_numpy(), va_result_df[pred_cols].to_numpy(), average=\"macro\")\n",
    "scores[\"final_metric\"] = score\n",
    "\n",
    "va_result_df.write_parquet(EXP_OUTPUT_DIR / \"va_result_df.parquet\")\n",
    "with open(EXP_OUTPUT_DIR / \"scores.json\", \"w\") as f:\n",
    "    json.dump(scores, f, indent=4)\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_result_df = pl.DataFrame()\n",
    "for i, (name, res) in enumerate(results.items()):\n",
    "    target_name = name.split(\"_\")[1]\n",
    "    if i == 0:\n",
    "        cols = [pl.col(\"session_id\"), pl.col(\"pred\").alias(target_name)]\n",
    "    else:\n",
    "        cols = [pl.col(\"pred\").alias(target_name)]\n",
    "\n",
    "    _te_result_df = single_inference_fn_v2(\n",
    "        models=res[\"models\"],\n",
    "        features_df=test_feature_df,\n",
    "        feature_names=feature_cols,\n",
    "    ).select(cols)\n",
    "    te_result_df = pl.concat([te_result_df, _te_result_df], how=\"horizontal\")\n",
    "\n",
    "submission_df = (\n",
    "    test_session_df.select(\"session_id\")\n",
    "    .join(te_result_df, on=\"session_id\", how=\"inner\")\n",
    "    .select([\"session_id\"] + TARGET_COLS)\n",
    ")\n",
    "\n",
    "submission_df.write_parquet(EXP_OUTPUT_DIR / \"te_result_df.parquet\")\n",
    "submission_df.select(\n",
    "    [\n",
    "        \"チョコレート\",\n",
    "        \"ビール\",\n",
    "        \"ヘアケア\",\n",
    "        \"米（5㎏以下）\",\n",
    "    ]\n",
    ").write_csv(EXP_OUTPUT_DIR / \"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
